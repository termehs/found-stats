# Estimation {#sec-est}
When working with data, we often face the challenge of estimating unknown characteristics of a population based on a limited set of observations: a sample. These characteristics could be the population mean ($\mu$), the population proportion ($p$), or the population variance ($\sigma^2$), among others. In general, we denote the unknown population parameter we wish to estimate by $\theta$.

Suppose we collect a random sample consisting of $n$ independent observations, denoted by $X_1, X_2, \dots, X_n$. Based on this sample, we compute an estimate of $\theta$, which we denote as $\hat{\theta}$. For instance, if the parameter of interest is the population mean $\mu$, then the corresponding estimate is the sample mean $\bar{X}$. If the parameter of interest is the population variance $\sigma^2$, the estimate is the sample variance $s^2$.

Because the sample is randomly drawn from the population, the estimate $\hat{\theta}$ is itself a random variable. Its value depends on the specific observations in the sample, and therefore it varies from sample to sample. The distribution of this estimate across many repeated samples is known as its sampling distribution (see @sec-sampldist-mean and @sec-sampldist-prop). Understanding the behavior of this distribution is central to inferential statistics: it allows us to assess the uncertainty in our estimates and build confidence intervals, conduct hypothesis tests, and more.

Naturally, we hope that the value of our estimate $\hat{\theta}$ is close to the true, unknown value of $\theta$. However, there is always a degree of uncertainty involved. To evaluate the quality of an estimator, we study its properties, most importantly, its expected value and its variance.



## Estimators, Estimate, Estimands

For clarity, it's important to distinguish between three closely related but conceptually distinct terms in inferential statistics: **estimand**, **estimator**, and **estimate**.

- **Estimand**: This is the **target** of our inference—the unknown quantity or parameter in the population we want to learn about. Examples include the population mean $\mu$, the proportion of voters supporting a policy $p$, or the difference in means between two groups $\mu_1 - \mu_2$.

- **Estimator**: A formula or rule that we apply to sample data in order to make an informed guess about the estimand. It is a **random variable**, as it depends on the data, which in turn vary across samples. For instance, the sample mean $\bar{X}$ is an estimator for the population mean $\mu$.

- **Estimate**: The **actual numerical result** obtained when we apply the estimator to a specific dataset. It is a **fixed number**, not a random variable. For example, if $\bar{X} = 7.4$ from our sample, then 7.4 is our estimate of the population mean.

In simpler terms:

| Term       | Role                            | Example                   |
|------------|----------------------------------|---------------------------|
| Estimand   | What we want to know             | $\mu$, $p$, $\mu_1 - \mu_2$ |
| Estimator  | How we calculate it              | $\bar{X}$, $\hat{p}$      |
| Estimate   | What we get from our data        | 7.4, 0.38                 |

> ⚠️ While these terms are sometimes used interchangeably in casual discussion, understanding their formal distinction is crucial for clear statistical reasoning.

In this text, we may occasionally use "estimate" loosely, but rest assured; we’ll always be clear about what we're inferring, how we're doing it, and what the result actually tells us.


## Properties of Estimators
An estimator's quality is assessed through certain desirable properties. The most fundamental ones are **unbiasedness**, **consistency**, and **efficiency**.

An estimator $\hat{\theta}$ is called **unbiased** if its expected value equals the true parameter:

$$
E(\hat{\theta}) = \theta.
$$

This means that, on average across many samples, the estimator hits the correct value. If this condition is not met, the estimator is said to have a **bias**, defined as:

$$
\text{Bias}(\hat{\theta}) = E(\hat{\theta}) - \theta.
$$

An estimator with zero bias is called unbiased. But just being correct *on average* is not enough. We also want our estimator to be **reliable**, that is, to give values that are close to the true parameter in most samples, not just on average.

This brings us to the property of **consistency**. An estimator is **consistent** if it gets arbitrarily close to the true parameter as the sample size increases. Formally, a consistent estimator $\hat{\theta}_n$ satisfies:

$$
\hat{\theta}_n \xrightarrow{p} \theta \quad \text{as } n \to \infty.
$$

That is, the probability that the estimator deviates substantially from $\theta$ goes to zero as the number of observations grows. Consistency ensures long-run accuracy with enough data.

Finally, among several unbiased and consistent estimators, we may prefer the one that tends to stay closest to the target in each sample. This is the idea behind **efficiency**. An estimator is more efficient if it has smaller variance among all unbiased estimators. In practical terms, this means it's more **precise**: it fluctuates less from sample to sample, producing estimates tightly clustered around the true value. When comparing two unbiased estimators of the same parameter, the one with the smaller variance is considered more efficient. 

To illustrate these concepts, consider the dartboard image below:

::: center
![Each board illustrates a combination of estimator characteristics. The bottom-left shows an unbiased and efficient estimator (tight clustering at the center). The top-left is biased but efficient (tight clustering, but off-target). The bottom-right is unbiased but inefficient (centered but scattered). The top-right represents an inefficient and biased estimator (wide scatter, off-target). This analogy helps visualize bias, precision, and the ideal goal of consistency over repeated samples.](imgs/dart.png){#fig-dart fig-align="center" width="70%"}
:::


Together, these properties give us a clear framework for evaluating the reliability of estimators. Ideally, we want an estimator that is unbiased and has the smallest possible variance, a combination that yields both accuracy and precision in estimation.




## Point Estimation

In statistical inference, we often aim to estimate unknown characteristics of a population—such as its average income, the proportion of voters in favor of a policy, or the variance in housing prices. This process of using data from a sample to calculate a single value as a “best guess” for a population parameter is known as **point estimation**.

Let us assume we have a random sample $X_1, X_2, \ldots, X_n$ of $n$ independent observations from a population with an unknown mean $\mu$ and variance $\sigma^2$. A **point estimator** is a function of the sample data used to estimate a population parameter.

For instance:

- The **population mean** $\mu$ is typically estimated by the **sample mean** $\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i$
- The **population proportion** $p$ is estimated by the **sample proportion** $\hat{p} = \frac{x}{n}$, where $x$ is the number of "successes"
- The **population variance** $\sigma^2$ is estimated by the **sample variance** $s^2 = \frac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2$

These estimators are derived from the data and yield a specific number once we collect our sample. However, prior to observing the data, the estimator is a random variable because it depends on the randomly selected sample.

Since estimators are functions of random samples, they too follow a probability distribution. This distribution is known as the sampling distribution of the estimator (see @sec-sampldist-mean and @sec-sampldist-prop). It tells us how the estimator would vary if we repeatedly took samples of the same size from the same population.

A crucial insight here is that the larger the sample size, the less variability the estimator will show across repeated samples, leading to more **reliable** estimation.

Understanding the quality of an estimator involves examining the theoretical properties mentioned above:

- **Unbiasedness**: An estimator $\hat{\theta}$ is said to be **unbiased** for a parameter $\theta$ if its expected value equals the true parameter:  
  $$ E(\hat{\theta}) = \theta $$

  For example (this was shown in detail in @sec-sampldist-mean and @sec-sampldist-prop):
  - $E(\bar{X}) = \mu$, so the sample mean is an unbiased estimator of the population mean.
  - $E(\hat{p}) = p$, meaning the sample proportion is also unbiased for the true proportion.
  - $E(s^2) = \sigma^2$, making $s^2$ an unbiased estimator of the population variance.

  *(Note: The sample standard deviation $s$ is not an unbiased estimator of $\sigma$.)*

- **Variance**: The **spread** of the estimator’s sampling distribution is captured by its variance. Smaller variance means the estimator tends to be closer to the true parameter.

  For the sample mean, this is given by:  
  $$ \operatorname{Var}(\bar{X}) = \frac{\sigma^2}{n} $$  
  Similarly, for the sample proportion:
  $$ \operatorname{Var}(\hat{p}) = \frac{p(1 - p)}{n} $$

  These expressions show that increasing the sample size $n$ decreases the variance of the estimator.

- **Standard Error**: The **standard deviation** of an estimator’s sampling distribution is called the **standard error** (SE). It quantifies how much the estimator would vary from sample to sample:
  $$ \text{SE}(\bar{X}) = \sqrt{\frac{\sigma^2}{n}}, \quad \text{SE}(\hat{p}) = \sqrt{\frac{p(1 - p)}{n}} $$

In practice, when $\sigma^2$ or $p$ are unknown, we often substitute their estimates.


Point estimation provides an intuitive way to infer unknown quantities in a population using observed sample data. It’s the first essential tool in the inferential statistics toolbox, allowing us to move from descriptive summaries to educated guesses about the world beyond our data. In the coming chapters, we will build on these estimators to construct confidence intervals and perform hypothesis testing, all grounded in the probabilistic behavior of these sampling distributions.

## Interval Estimation

In statistics, a single number, known as a point estimate, can give us an estimate of a population parameter, like the mean. But such an estimate tells us little about how precise or reliable it is. After all, due to the randomness inherent in sampling, our estimate could vary from sample to sample. That’s why we turn to **interval estimation**.

A **confidence interval** provides a range of plausible values for the unknown parameter. It gives a way to quantify the uncertainty of our point estimate. The general form of a confidence interval can be written as:

$$
\text{Point Estimate} \pm \text{Margin of Error}
$$

More formally, a confidence interval is constructed so that, with a specified probability known as the **confidence level**, contains the true value of the parameter. This probability is denoted by $1 - \alpha$, where $\alpha$ is the **significance level**. Common choices are 95% confidence ($\alpha = 0.05$) or 99% confidence ($\alpha = 0.01$). The higher the confidence level, the wider the interval must be to ensure it covers the true parameter more frequently in repeated samples.

While a point estimate gives us a single best guess for the value of a population parameter, it offers no information about the uncertainty of that guess. This is visualized in @fig-fish. Imagine you’re trying to catch a fish (think *parameter*) you can’t quite see. Point estimation is like throwing a spear: accurate if you hit, but there's a good chance you'll miss. Interval estimation is more forgiving: it's like casting a net. You may not catch the exact center, but you're much more likely to get the fish. This is exactly what confidence intervals offer: a wider, more reliable shot at capturing the truth.






::: center
![An analogy for estimation methods: the top panel shows point estimation as attempting to catch a fish with a spear: precise but risky. The bottom panel illustrates interval estimation as casting a wide net: less precise but with a greater chance of capturing the true value.](imgs/point-interval-fish.png){#fig-fish fig-align="center" width="40%"}
:::

### Confidence Interval for the Population Mean {.unnumbered}
When we talk about a **confidence interval** for the population mean $\mu$ with a **confidence level** of 95%, we mean an interval that, in the long run, will contain the true value of $\mu$ in 95% of repeated samples. In practice, we never know the true mean, but we use our sample data to construct a range where we believe the mean is likely to lie.

More precisely, the endpoints of the interval (called the *Lower Confidence Limit* (LCL) and *Upper Confidence Limit* (UCL)) are calculated as:

$$
\begin{aligned}
\text{LCL} &= \bar{X} - z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}, \\
\text{UCL} &= \bar{X} + z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}
\end{aligned}
$$

For a 95% confidence level, the critical value is $z_{\alpha/2} \approx 1.96$.


Before we observe any data, the sample mean $\bar{X}$ is a random variable. If the population is **normally distributed with known variance $\sigma^2$**, then:

$$
\bar{X} \sim \mathcal{N}(\mu, \frac{\sigma^2}{n})
$$

Standardizing gives:

$$
Z = \frac{\bar{X} - \mu}{\sigma/\sqrt{n}} \sim \mathcal{N}(0, 1)
$$

We know (from @sec-normaltbl):

$$
P(-1.96 \leq Z \leq 1.96) = 0.95
$$

Rewriting this inequality in terms of $\mu$:

$$
P\left(\underbrace{\bar{X} - 1.96 \cdot \frac{\sigma}{\sqrt{n}}}_{\text{LCL}} \leq \mu \leq  \underbrace{\bar{X} + 1.96 \cdot \frac{\sigma}{\sqrt{n}}}_{\text{UCL}}\right) = 0.95
$$

This tells us that if we repeat this sampling process many times, 95% of the resulting intervals will contain the true population mean. This is visualized in @fig-cfi.

::: {#fig-cfi}
```{r}
#| fig-height: 7
#| fig-width: 6
#| fig-align: center

library(ggplot2)
library(patchwork)
# Base plot setup for standard normal
 p1 <-  ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), 
                linewidth = 0.5, color = "black") +
  geom_area(stat = "function", fun = dnorm, 
            args = list(0, 1), 
            fill = "skyblue", alpha = 0.5,
            xlim = c(-1.96, 1.96)) +
  
  # Vertical lines at -1.96 and 1.96
  geom_vline(xintercept = c(-1.96, 1.96), linetype = "dashed", color = "firebrick") +
  
  # Text in the middle
  annotate("text", x = 0, y = 0.15, 
           label = expression(1 - alpha == 0.95), size = 5) +
  
  # Arrow labels for alpha/2
  annotate("text", x = -3, y = 0.05, 
           label = expression(alpha/2 == 0.025), size = 4) +
  annotate("segment", x = -2.8, xend = -2.2, y = 0.04, yend = 0.01,
           arrow = arrow(length = unit(0.1, "inches")), size = 0.3) +
  
  annotate("text", x = 3, y = 0.05, 
           label = expression(alpha/2 == 0.025), size = 4) +
  annotate("segment", x = 2.8, xend = 2.2, y = 0.04, yend = 0.01,
           arrow = arrow(length = unit(0.1, "inches")), size = 0.3) +

  # Labels for z-values
  annotate("text", x = -2.8, y = -0.015, 
           label = expression(-z[alpha/2] == -1.96), size = 4, vjust = 1, color = "firebrick") +
  annotate("text", x = 2.65, y = -0.015, 
           label = expression(z[alpha/2] == 1.96), size = 4, vjust = 1, color = "firebrick") +

  # Mean label
  annotate("text", x = 0, y = -0.015, 
           label = expression(mu == 0), size = 4, vjust = 1) +

  theme_minimal(base_size = 14) +
  theme(
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 16)
  )
  
p2 <-  ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  # Density curve
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), 
                linewidth = 0.5, color = "black") +
  
  # Shaded 95% region
  geom_area(stat = "function", fun = dnorm,
            args = list(mean = 0, sd = 1),
            fill = "skyblue", alpha = 0.5,
            xlim = c(-1.96, 1.96)) +
  
  # Vertical lines at -1.96 and 1.96
  geom_vline(xintercept = c(-1.96, 1.96), color = "firebrick", linetype = "dashed") +
  
  # Main CI annotation
  annotate("text", x = 0, y = 0.15, 
           label = expression(1 - alpha == 0.95), size = 5) +
  
# Arrow labels for alpha/2
  annotate("text", x = -3, y = 0.05, 
           label = expression(alpha/2 == 0.025), size = 4) +
  annotate("segment", x = -2.8, xend = -2.2, y = 0.04, yend = 0.01,
           arrow = arrow(length = unit(0.1, "inches")), size = 0.3) +
  
  annotate("text", x = 3, y = 0.05, 
           label = expression(alpha/2 == 0.025), size = 4) +
  annotate("segment", x = 2.8, xend = 2.2, y = 0.04, yend = 0.01,
           arrow = arrow(length = unit(0.1, "inches")), size = 0.3) +

  # LCL and UCL formulas below x-axis
  annotate("text", x = -3.2, y = -0.015,
           label = expression("LCL = " * bar(X) - 1.96 * (sigma / sqrt(n))),
           size = 4, vjust = 1,color = "firebrick") +
  annotate("text", x = 3.3, y = -0.015,
           label = expression("UCL = " * bar(X) + 1.96 * (sigma / sqrt(n))),
           size = 4, vjust = 1,color = "firebrick") +

  # Mean label
  annotate("text", x = 0, y = -0.015, 
           label = expression(mu), size = 4, vjust = 1) +

  theme_minimal(base_size = 14) +
  theme(
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 16)
  )

p2/p1
```

Visual comparison of a 95% confidence interval (top) and the corresponding central probability region of the standard normal distribution (bottom). The top plot shows the confidence interval expressed in terms of the sample mean $\bar{X}$ and standard error, while the bottom plot uses the standardized $z$-values. In both cases, the central 95% of the distribution is shaded, with $\alpha/2 = 0.025$ in each tail, and the total area between the bounds representing 95% probability.
:::

Each time we draw a new sample from a population and construct a confidence interval for the population mean $\mu$, the endpoints of that interval are random; they depend on the data from that specific sample. This means that the interval can change from sample to sample. However, if we use a 95% confidence level, then in the long run, 95% of those intervals will capture the true mean, and 5% will not.

In other words, we can’t guarantee that a single confidence interval contains the population mean, but we can say that the method used to construct it is correct 95% of the time. To show this, we perform a simulation: we simulate 100 samples from the same population, construct a 95% confidence interval for each, and visualizes which ones do and do not include the true population mean. Each vertical line in  @fig-cfi-sim represents one confidence interval from a sample of size $n = 30$. The blue line shows the true mean $\mu = 5$ and. The grey intervals contain the true mean, as expected 95% of the time. Red intervals are the $\approx$5% that miss, a natural consequence of statistical variation.


::: {#fig-cfi-sim}
```{r}
#| fig-height: 5
#| fig-width: 8
#| fig-align: center
#| echo: false

library(dplyr)

set.seed(2023)

# Parameters
mu <- 5                # true population mean
sigma <- 2             # true population sd
n <- 30                # sample size
nsim <- 100            # number of intervals

# Generate intervals
sim_data <- replicate(nsim, {
  sample <- rnorm(n, mean = mu, sd = sigma)
  xbar <- mean(sample)
  se <- sigma / sqrt(n)
  lower <- xbar - 1.96 * se
  upper <- xbar + 1.96 * se
  c(mean = xbar, lower = lower, upper = upper)
})

sim_df <- as.data.frame(t(sim_data))
sim_df <- sim_df %>%
  mutate(id = row_number(),
         includes_mu = (lower <= mu & upper >= mu))

# Plot
ggplot(sim_df, aes(x = id, y = mean, ymin = lower, ymax = upper)) +
  geom_errorbar(color = ifelse(sim_df$includes_mu, "grey40", "indianred1"), width = 0.2) +
  geom_point(color ="grey40") +
  geom_hline(yintercept = mu, color = "#41b6c4", linetype = "solid", linewidth = 0.5) +
annotate("text", x = 102, y = 4.98, 
           label = expression(mu), size = 7, vjust = 1, color = "#41b6c4") +
  labs(x = "Sample index",
       y = "Confidence interval") +
  theme_minimal()
```

Simulation of 100 confidence intervals for the population mean $\mu = 5$. Each vertical line represents a 95% confidence interval constructed from a random sample of size $n = 30$. Grey intervals successfully capture the true mean, while red intervals do not, illustrating the expected 5% miss rate due to sampling variability. 
:::


So how do we interpret the confidence interval? Before we collect any data, we can say that there is a 95% probability that the interval we are about to compute will contain the true population mean, $\mu$. This probability statement refers to the process, not the specific interval. That is, we are 95% confident in the method, not in any one result. Once a sample is drawn and the confidence interval is computed, the situation changes. The interval is now fixed, it either contains $\mu$ or it doesn’t. We no longer speak of probabilities regarding this specific interval. However, what we do know is this:

> The interval was calculated using a procedure that, in the long run, produces intervals that contain the true mean in 95% of all cases.

This is the essence of the frequentist interpretation of confidence intervals. It doesn’t tell us the probability that our specific interval contains $\mu$, but it does provide a measure of trust in the procedure we used to create it. That’s why we refer to it as a “confidence” interval — not because we are certain, but because we have reason to be confident based on the method’s performance over repeated samples.

So far, we have only constructed 95% confidence intervals for a population mean. This means that if we were to repeat the sampling procedure many times, approximately 95% of those intervals would contain the true population mean, $\mu$.

But 95% is not a fixed rule. We can choose other confidence levels—such as 90% or 99%—depending on how certain we want to be. The trade-off is that a *higher level* of confidence results in a *wider interval*, while a *lower confidence level* gives a *narrower but less certain estimate*.

When the population standard deviation $\sigma$ is known and the sampling distribution of the mean is approximately normal (either by assumption or via the Central Limit Theorem), the general structure of a confidence interval for the mean is:

$$
\bar{X} \pm z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}
$$

where:

- $\bar{X}$ is the sample mean  
- $\sigma$ is the population standard deviation  
- $n$ is the sample size  
- $z_{\alpha/2}$ is the critical value from the standard normal distribution for the desired confidence level


Some common confidence levels and their corresponding $z$-values are as follows (see @sec-normaltbl):

| Confidence Level | $z_{\alpha/2}$ |
|------------------|----------------|
| 90%              | 1.645   (1.65)      |
| 95%              | 1.960   (1.96)      |
| 99%              | 2.575   (2.58)      |

As seen in @fig-cfi-2, a higher confidence level (e.g., 99% instead of 95%) leads to a wider interval since the $z$ value moves further away from the center towards the tails, giving us more “certainty” at the cost of less precision. A larger sample size reduces the standard error $\frac{\sigma}{\sqrt{n}}$, resulting in a narrower interval, i.e., better precision without sacrificing confidence.

::: {#fig-cfi-2}
```{r}
#| label: fig-confidence-interval
#| fig-height: 4
#| fig-width: 7
#| fig-align: center

ggplot(data.frame(x = c(-4, 4)), aes(x = x)) +
  stat_function(fun = dnorm, args = list(mean = 0, sd = 1), 
                linewidth = 0.5, color = "black") +
  geom_area(stat = "function", fun = dnorm, 
            args = list(0, 1), 
            fill = "skyblue", alpha = 0.5,
            xlim = c(-1.96, 1.96)) +
  
  # Vertical lines at -1.96 and 1.96
  geom_vline(xintercept = c(-1.96, 1.96), linetype = "dashed", color = "firebrick") +
  
  # Text in the middle
  annotate("text", x = 0, y = 0.15, 
           label = expression(1 - alpha), size = 5) +
  
  # Arrow labels for alpha/2
  annotate("text", x = -3, y = 0.05, 
           label = expression(alpha/2 == 0.025), size = 4) +
  annotate("segment", x = -2.8, xend = -2.2, y = 0.04, yend = 0.01,
           arrow = arrow(length = unit(0.1, "inches")), size = 0.3) +
  
  annotate("text", x = 3, y = 0.05, 
           label = expression(alpha/2 == 0.025), size = 4) +
  annotate("segment", x = 2.8, xend = 2.2, y = 0.04, yend = 0.01,
           arrow = arrow(length = unit(0.1, "inches")), size = 0.3) +

  # Labels for z-values
  annotate("text", x = -2.3, y = -0.015, 
           label = expression(-z[alpha/2]), size = 4, vjust = 1, color = "firebrick") +
  annotate("text", x = 2.25, y = -0.015, 
           label = expression(z[alpha/2]), size = 4, vjust = 1, color = "firebrick") +

  # Mean label
  annotate("text", x = 0, y = -0.015, 
           label = expression(mu == 0), size = 4, vjust = 1) +

  theme_minimal(base_size = 14) +
  theme(
    axis.title = element_blank(),
    axis.text = element_blank(),
    axis.ticks = element_blank(),
    plot.title = element_text(hjust = 0.5, size = 16)
  )
```
The confidence interval spans the central $(1 - \alpha)$ area of the standard normal distribution. Each tail has area $\alpha/2$.
:::

> 💡 *A higher confidence level leads to a wider interval: you're more confident, but less precise*.


This approach provides a principled way of capturing the uncertainty in our estimation of $\mu$. While the point estimate $\bar{X}$ gives our best guess, the confidence interval gives us a sense of how variable that estimate might be if we repeated the experiment.


In the following, we continue to build on this framework and explore how confidence intervals can be constructed for other types of parameters and under various assumptions.


#### Example 21.1: Confidence Interval for a Population Mean {.unnumbered}
Let's consider a scenario where we take a random sample of size $n = 25$ from a normally distributed population. Suppose we know the population standard deviation to be $\sigma = 15$, and the sample mean is $\bar{x} = 102$.

We want to construct a 95% confidence interval for the population mean $\mu$. We use the formula for a confidence interval when the population standard deviation is known:

$$
\bar{x} \pm z_{\alpha/2} \cdot \frac{\sigma}{\sqrt{n}}
$$

For a 95% confidence level, the critical value from the standard normal distribution is $z_{0.025} = 1.96$.

Let's plug in the values:

$$
102 \pm 1.96 \cdot \frac{15}{\sqrt{25}} = 102 \pm 1.96 \cdot 3 = 102 \pm 5.88
$$

Hence, the 95% confidence interval becomes:

$$
(96.12,\ 107.88)
$$

This interval suggests that if we were to repeat this sampling process many times, approximately 95% of the resulting intervals would contain the true population mean $\mu$.

*What if we want a 99% confidence interval?*
A higher confidence level requires a larger critical value. For 99% confidence, we use $z_{0.005} = 2.575$. Applying the formula:

$$
102 \pm 2.575 \cdot \frac{15}{\sqrt{25}} = 102 \pm 2.575 \cdot 3 = 102 \pm 7.725
$$

This results in a wider interval:

$$
(94.275,\ 109.725)
$$

As expected, increasing the confidence level results in a wider interval. This reflects greater certainty, more room is allowed to ensure the true mean is captured.


### Confidence Intervals with Unknown Variance {.unnumbered}

In previous examples, we constructed confidence intervals assuming that the population standard deviation $\sigma$ is known and that the population is normally distributed. While this is convenient for illustrating the basic logic of confidence intervals, it is rarely the case in practice.

>But what if $\sigma$ is unknown?

When $\sigma$ is **unknown**, we typically rely on the **sample standard deviation** $s$ instead. This introduces an additional source of uncertainty because we are now estimating both the center ($\bar{X}$) and the spread ($s$) from the sample data.

As a result, our confidence interval becomes wider, especially when the sample size $n$ is small. To account for this, we use the **$t$-distribution** rather than the standard normal distribution.

::: {.callout-note}
## Introducing the $t$-distribution {.unnumbered}
The $t$-distribution is a probability distribution used when we estimate the population mean from a small sample, especially when the population standard deviation is unknown. It looks similar to the standard normal distribution but has heavier tails, reflecting the greater uncertainty due to using the sample standard deviation. The shape of the $t$-distribution depends on the degrees of freedom (usually $n - 1$), and as the sample size grows, the $t$-distribution approaches the normal distribution (see @fig-tz-dist). This distribution is essential when constructing confidence intervals or performing hypothesis testing under these conditions.

::: {#fig-tz-dist}
```{r}
#| fig-height: 5
#| fig-width: 10

library(viridis)

# Create x-axis range
x <- seq(-4, 4, length.out = 400)

# Degrees of freedom for t-distributions
dfs <- c(1, 5, 10)

# Generate data for t-distributions
t_data <- lapply(dfs, function(df) {
  data.frame(
    x = x,
    density = dt(x, df),
    distribution = paste0("t (df = ", df, ")")
  )
}) |> bind_rows()

# Add standard normal distribution
normal_data <- data.frame(
  x = x,
  density = dnorm(x),
  distribution = "Standard Normal"
)

# Combine all and define order manually (normal first = darkest)
dist_data <- bind_rows(normal_data, t_data) |>
  mutate(distribution = factor(distribution, levels = c(
    "Standard Normal", "t (df = 1)", "t (df = 5)", "t (df = 10)"
  )))

# Plot using reversed viridis palette
ggplot(dist_data, aes(x = x, y = density, color = distribution)) +
  geom_line(linewidth = 1.2) +
  scale_color_viridis_d(option = "inferno", direction = 1) +
  labs( x = " ", y = " ", color = "Distribution"
  ) +
  theme_minimal(base_size = 14)
```
Comparison of the standard normal distribution (black) with Student’s $t$-distributions for degrees of freedom 1, 5, and 10. As the degrees of freedom increase, the $t$-distribution becomes more peaked and approaches the shape of the normal distribution, reflecting reduced uncertainty due to larger sample sizes.
:::

:::
