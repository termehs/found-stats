# Hypothesis Testing
In the previous chapter, we used **confidence intervals** to estimate an unknown population parameter such as the mean ($\mu$) or proportion ($p$), based on a random sample. Confidence intervals provide a range of plausible values for that parameter.

While confidence intervals focus on estimation, **hypothesis testing** addresses decision-making: it allows us to test specific claims about a population parameter using sample data.   In fact, these two methods are closely related; we can often reach the same conclusions using either approach. We return to this in more detail later in this chapter.


## What Is Hypothesis Testing?
Hypothesis testing is a formal method used to evaluate **two competing statements** (called hypotheses) about a population parameter:

- **Null hypothesis** ($H_0$): The default or status quo assumption.
- **Alternative hypothesis** ($H_1$ or $H_a$): A competing claim that we seek evidence for.

We use data from a sample to decide whether there is sufficient evidence to reject the null hypothesis in favor of the alternative. The null hypothesis ($H_0$) is assumed to be true unless the evidence from the sample strongly contradicts it. It plays the role of a "presumption of innocence." The alternative hypothesis ($H_1$) is what we hope to support, but only if we have enough evidence to doubt $H_0$.  


We never prove the alternative hypothesis directly; we can only reject $H_0$ if the evidence is strong enough, just like a jury does not prove guilt, but rather rejects the assumption of innocence when the evidence demands it. Rejecting $H_0$ only indicates that the data are inconsistent with $H_0$ under the assumed conditions.

#### Example 22.1: Bottled Water Production {.unnumbered}

A company bottles 500 ml of spring water per bottle. To ensure customer trust, they need to verify that the filling process remains accurate. Let

$$
\mu = \text{true average amount of water per bottle}
$$

We want to test whether the bottling machine is still calibrated correctly:

$H_0$: $\mu = 500$ (machine is accurate)

$H_1$: $\mu \neq 500$ (machine is underfilling or overfilling)


That is, the alternative hypothesis says: *"The machine does not have the correct precision."*

#### Example 22.2:  Political Party Support {.unnumbered}

In a political opinion poll, 14% of the selected individuals say they support Party A. We know that in the most recent election, Party A received 12% of the votes. Has the proportion of A-supporters in the population increased since the election?

$H_0$: $P = 0.12$ (The proportion is unchanged)  

$H_1$: $P > 0.12$ (The proportion has increased)

*Should we reject $H_0$ or not?*

- If we consider the high sample value to be explainable by chance (assuming $H_0$ is true), we stick with $H_0$. 
- If we consider the sample proportion to be too high to be reasonably explained by chance, we reject $H_0$.

We can never be 100% certain that we are making the correct decision. Statistical hypothesis testing involves using specific decision rules to determine when we should reject $H_0$ (and when we should retain $H_0$). These decision rules are designed so that we have a certain level of control over the risk of making an incorrect decision.

To understand the reasoning behind hypothesis testing, it can be helpful to draw an analogy from the legal system. Imagine a courtroom trial where the task is to determine whether the defendant is guilty or not. A key question in this context is: on whom does the burden of proof lie?

Just as in most legal systems, where a person is presumed innocent until proven guilty, hypothesis testing begins with a similar assumption. The **null hypothesis**, denoted $H_0$, plays the role of "innocence": it is the claim we initially assume to be true. The **alternative hypothesis**, denoted $H_1$, corresponds to the prosecution's claim: it challenges the status quo and must be supported by strong evidence.

There is an asymmetry in how we treat the null hypothesis ($H_0$) and the alternative hypothesis ($H_1$) in statistical hypothesis testing. We typically choose $H_0$ to be the hypothesis that we hold on to as long as possible. It is the default assumption, often representing "no change," "no effect," or "no difference." On the other hand, $H_1$ is usually the more bold and interesting hypothesis from an applied perspective.

We require particularly strong evidence from the observed data to reject $H_0$. The burden of proof lies with the party advocating for $H_1$. Before diving into how to best use information from the sample, we ask ourselves: how confident must we be that the accused is guilty before reaching a guilty verdict? Similarly, how confident must we be that the null hypothesis is incorrect before deciding to believe in the alternative hypothesis?

If we reach that level of confidence, we say that we reject the null hypothesis (or accept the alternative hypothesis). The degree of confidence required depends on the context, but typically, we want to be quite certain that the assumption in the null hypothesis is incorrect. This is because the decision to reject the null hypothesis, and instead believe in the alternative, often comes with serious consequences.


## Errors and Decision Framework
When making a decision, one either makes a correct choice or one of two types of errors. Just as there is a risk that an innocent person may be wrongly convicted, there is also a risk that a guilty person may go free.

Using probability theory, we can to some extent determine the risk of making incorrect decisions.

- A **Type I error** occurs when we incorrectly reject the null hypothesis ($H_0$) even though it is true. The risk of making a Type I error is called the **significance level** of the test and is denoted by the Greek letter $\alpha$.

- A **Type II error** occurs when we incorrectly fail to reject the null hypothesis, even though it is false. The risk of this error is denoted by the Greek letter $\beta$.

A diagram describing the possible outcomes of hypothesis testing is shown below:

| **Reality**      | $H_0$ is true              | $H_0$ is false             |
|------------------|----------------------------|----------------------------|
| **Decision**     |                            |                            |
| Do not reject $H_0$ | Correct decision           | Type II error (incorrect decision) |
| Reject $H_0$     | Type I error (incorrect decision) | Correct decision           |

In classical hypothesis testing, we primarily control the probability of making a Type I error. This is done by setting the significance level $\alpha$ to a predefined low value (commonly $\alpha = 0.01$, $0.05$, or $0.10$).

$$
\alpha = \text{Significance level of the test} = P(\text{Reject } H_0 \mid H_0 \text{ is true})
$$

After setting the acceptable level for Type I errors, we then aim to reduce the probability of Type II errors ($\beta$) by choosing a sufficiently large sample size.

## Classical Hypothesis Testing  

The classical approach to hypothesis testing involves the following six main steps:

### Step 1. Hypotheses {.unnumbered}

Begin by formulating the null hypothesis ($H_0$) and the alternative hypothesis ($H_1$).

Examples of hypotheses about population means:

- $H_0: \mu = \mu_0$; $H_1: \mu \ne \mu_0$  
  *(Simple null hypothesis; two-sided alternative)*

- $H_0: \mu \le \mu_0$; $H_1: \mu > \mu_0$  
  *(Composite null hypothesis; one-sided alternative)*

Examples of hypotheses about population proportions:

- $H_0: p = p_0$; $H_1: p \ne p_0$  
  *(Simple null hypothesis; two-sided alternative)*

- $H_0: p = p_0$; $H_1: p > p_0$  
  *(Simple null hypothesis; one-sided alternative)*

### Step 2. Significance Level {.unnumbered}

Choose a significance level $\alpha$, which represents the probability (or risk) of rejecting the null hypothesis $H_0$ when it is actually true.

A common choice is $\alpha = 0.05$, which implies that the hypothesis test is conducted at the 5% level.  
This means that there is a 5% chance of making a **Type I error**; rejecting a true null hypothesis.

In other words, if the null hypothesis is true, then on average, 1 in every 20 tests will incorrectly reject it.

If we want to be more cautious about rejecting a correct null hypothesis, we use a smaller $\alpha$, such as $\alpha = 0.01$.

### Step 3. Test Statistic {.unnumbered}

Specify which test statistic will be used.  
A **test statistic** is a quantity calculated from the sample data and its value forms the basis of our decision (see step 5).

The choice of test statistic depends on:

- Whether the sample is large or not.
- Whether the population is normally distributed or not.
- Whether the population variance is known or not.

We'll return to these different cases in more detail below.

## Step 4: Decision Rule  {.unnumbered}

Specify the **rejection region** (the critical region), such that $H_0$ is rejected if the test statistic falls within this region.

In summary, the form of the **critical region** is determined by:

1. The form of the alternative hypothesis:

   - $H_1: \mu \neq \mu_0$ → two-tailed test → rejection region in both tails.
   - $H_1: \mu > \mu_0$ → one-tailed test → rejection region in the **right** tail.
   - $H_1: \mu < \mu_0$ → one-tailed test → rejection region in the **left** tail.

2. The significance level $\alpha$.

Alternative hypotheses of the forms $>$ and $<$ are called **one-sided**, while those using $\neq$ are called **two-sided**. @Fig-hyptest illustrates the decision framework of hypothesis testing  three panels: 

1.	Left: Depicts a two-tailed test, where both tails of the distribution represent critical regions. The dashed lines mark the critical values at $z = \pm z_{\alpha/2}$. Each tail has an area of $\alpha/2$, and the null hypothesis is rejected if the test statistic lies in either shaded tail. The central blue segment indicates the non-rejection region.  

2.	Right: Represents a one-sided test with the alternative hypothesis $H_1: \mu > \mu_0$. The right tail is shaded red, indicating the critical region with area $\alpha$. The test statistic must fall in this region to reject $H_0$.

3.	Middle: Shows the critical region in the left tail of the distribution, representing a one-sided test with the alternative hypothesis $H_1: \mu < \mu_0$. The red-shaded area corresponds to the significance level $\alpha$, and any test statistic falling in this region leads to rejection of the null hypothesis $H_0$.

::: {#fig-hyptest .center}

```{r}
#| warning: false
#| message: false
#| fig-width: 12
#| fig-height: 3
#| fig-align: center

library(ggplot2)
library(patchwork)

# Data for standard normal curve
x_vals <- seq(-4, 4, length.out = 1000)
df <- data.frame(x = x_vals, y = dnorm(x_vals))

# Define alpha level and critical values
alpha <- 0.05
crit_val <- qnorm(1 - alpha / 2)  # ≈ ±1.96

# Data for shaded critical regions
df_left <- subset(df, x <= -crit_val)
df_right <- subset(df, x >= crit_val)

# Find y-values at critical points
y_crit <- dnorm(crit_val)

# Base plot
p1 <- ggplot(df, aes(x = x, y = y)) +
  geom_line(color = "black", linewidth = 0.7) +

  # Shade the critical regions
  geom_area(data = df_left, fill = "red", alpha = 0.5) +
  geom_area(data = df_right, fill = "red", alpha = 0.5) +

  # Vertical lines at critical values that stop at the curve
  annotate("segment", x = -crit_val, xend = -crit_val, y = 0, yend = y_crit, linetype = "solid", color = "black") +
  annotate("segment", x = crit_val, xend = crit_val, y = 0, yend = y_crit, linetype = "solid", color = "black") +

  # Horizontal colored lines for critical and acceptance regions
  annotate("segment", x = -4, xend = -crit_val, y = 0, yend = 0, color = "red", linewidth = 2) +
  annotate("segment", x = -crit_val, xend = crit_val, y = 0, yend = 0, color = "deepskyblue3", linewidth = 2) +
  annotate("segment", x = crit_val, xend = 4, y = 0, yend = 0, color = "red", linewidth = 2) +

   # Arrows pointing into tails
  annotate("segment", x = -crit_val-0.6, xend = -crit_val-0.15, y = 0.07, yend = 0.02,
           arrow = arrow(length = unit(0.18, "cm")), color = "black") +
  annotate("segment", x = crit_val + 0.6, xend = crit_val + 0.15, y = 0.06, yend = 0.02,
           arrow = arrow(length = unit(0.18, "cm")), color = "black") +

  # Annotate regions
  annotate("text", x = 0, y = 0.44, label = "Two-sided alternative hypothesis", size = 5, fontface = "italic") +
  annotate("text", x = 0, y = -0.03, label = "Non-rejection region", size = 4.2) +
  annotate("text", x = -3.1, y = -0.03, label = "Critical region", size = 3.8, color = "firebrick") +
  annotate("text", x = 3.1, y = -0.03, label = "Critical region", size = 3.8, color = "firebrick") +
  annotate("text", x = -crit_val-0.7, y = 0.1, label = "α/2", color = "black", size = 4, vjust = 1.2) +
  annotate("text", x = crit_val+0.7, y = 0.09, label = "α/2", color = "black", size = 4, vjust = 1.2) +

  # Clean theme
  labs(x = NULL, y = NULL) +
  theme_void() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  )

#  plot
p2 <- ggplot(df, aes(x = x, y = y)) +
  geom_line(color = "black", linewidth = 0.7) +

  # Shade the critical regions
  geom_area(data = df_right, fill = "red", alpha = 0.5) +

  # Vertical  lines at critical values that stop at the curve

  annotate("segment", x = crit_val, xend = crit_val, y = 0, yend = y_crit, linetype = "solid", color = "black") +

  # Horizontal colored lines for critical and acceptance regions

  annotate("segment", x = -4, xend = crit_val, y = 0, yend = 0, color = "deepskyblue3", linewidth = 2) +
  annotate("segment", x = crit_val, xend = 4, y = 0, yend = 0, color = "red", linewidth = 2) +

   # Arrows pointing into tails
  annotate("segment", x = crit_val + 0.6, xend = crit_val + 0.15, y = 0.06, yend = 0.02,
           arrow = arrow(length = unit(0.18, "cm")), color = "black") +

  # Annotate regions
  annotate("text", x = 0, y = 0.44, label = "One-sided alternative hypothesis (>)", size = 5, fontface = "italic") +
  annotate("text", x = -0.5, y = -0.03, label = "Non-rejection region", size = 4.2) +
  annotate("text", x = 3.1, y = -0.03, label = "Critical region", size = 3.8, color = "firebrick") +
  annotate("text", x = crit_val+0.7, y = 0.09, label = "α", color = "black", size = 4, vjust = 1.2) +

  # Clean theme
  labs(x = NULL, y = NULL) +
  theme_void() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  )

# plot

p3 <- ggplot(df, aes(x = x, y = y)) +
  geom_line(color = "black", linewidth = 0.7) +

  # Shade the critical regions
  geom_area(data = df_left, fill = "red", alpha = 0.5) +

  # Vertical lines at critical values that stop at the curve
  annotate("segment", x = -crit_val, xend = -crit_val, y = 0, yend = y_crit, linetype = "solid", color = "black") +


  # Horizontal colored lines for critical and acceptance regions
  annotate("segment", x = -4, xend = -crit_val, y = 0, yend = 0, color = "red", linewidth = 2) +
  annotate("segment", x = -crit_val, xend = 4 , y = 0, yend = 0, color = "deepskyblue3", linewidth = 2) +


   # Arrows pointing into tails
  annotate("segment", x = -crit_val-0.6, xend = -crit_val-0.15, y = 0.07, yend = 0.02,
           arrow = arrow(length = unit(0.18, "cm")), color = "black") +


  # Annotate regions
  annotate("text", x = 0, y = 0.44, label = "Two-sided alternative hypothesis (<)", size = 5, fontface = "italic") +
  annotate("text", x = 0.5, y = -0.03, label = "Non-rejection region", size = 4.2) +
  annotate("text", x = -3.1, y = -0.03, label = "Critical region", size = 3.8, color = "firebrick") +
  annotate("text", x = -crit_val-0.7, y = 0.1, label = "α", color = "black", size = 4, vjust = 1.2) +
  # Clean theme
  labs(x = NULL, y = NULL) +
  theme_void() +
  theme(
    axis.text.y = element_blank(),
    axis.ticks = element_blank(),
    panel.grid = element_blank()
  )

p1+ p2+ p3
```
The decision framework of hypothesis testing for two-tailed and one-tailed tests.
:::

Once the hypotheses have been formulated, the appropriate test statistic selected and decision rules specified, we proceed with the final steps of the classical hypothesis testing procedure:

## Step 5: Observation {.unnumbered}

We calculate the value of the **test statistic** using the data obtained from the sample. This computation allows us to compare the observed value with the theoretical distribution *under the null hypothesis*. This value is what we refere to as our **observed value**.

## Step 6: Conclusion  {.unnumbered}

Based on the value of the test statistic (our observed value), we make our decision based on the decision rule:

- If the value falls **outside the critical boundaries** (i.e. outside the non-rejection region), we **reject the null hypothesis** $H_0$. This means that we have obtained a result that is statistically significant at the chosen significance level $\alpha$.

- If the value falls **within the non-rejection region**, we **do not reject** $H_0$. In this case, the result is said to be **not statistically significant**.

In essence, these final steps guide us in deciding whether the sample data provides enough evidence to conclude that the null hypothesis is unlikely to be true, given the selected confidence level.


::: {.callout-note}
## Note
A non-significant result does **not** mean that we can conclude the null hypothesis ($H_0$) is true. It simply indicates that the alternative hypothesis ($H_1$) does not present strong enough evidence against $H_0$ in this particular case.

There may be many other potential null hypotheses that would also not be rejected. Therefore, **failing to reject $H_0$ is not the same as accepting $H_0$ as true**.

In other words, we use the terms ‘*reject*’ and ‘*fail to reject*’ to summarize the possible outcomes of a hypothesis test
:::
