# What is Statistics?

Statistics is all about making informed decisions in an uncertain world. Imagine you‚Äôre a detective piecing together clues (data) to solve a case (hypothesis testing). But instead of clear-cut evidence, you get noisy, incomplete, and sometimes misleading information.

At its core, statistics is *the science of changing your mind under uncertainty*‚Äî not because you‚Äôre indecisive, but because data has the power to prove you wrong! Making decisions without data is like guessing the weather based on your mood. Data helps us navigate randomness, update our beliefs, and make smarter choices based on evidence rather than gut feeling. You're basically saying, we don‚Äôt know everything, but let‚Äôs make the best of what we‚Äôve got. 

Using statistical terminology (*in parenthesis*) we can sunmmarize as following: Making decisions based on facts means having information about all the facts (*parameters*) . But most of the time we don‚Äôt have all the information, and what we know comes (*sample*)  is often different than what we wish we knew (*population*). So we guess (*estimate*) under uncertainty.

The uncertainty comes from the fact that the world is messy, unpredictable, and full of unknowns. No data is perfect, randomness exist everywhere. Further to this, even if we collect tons of data, there‚Äôs always some level of error. Instead of making absolute claims, statistics helps us express how confident we are in our conclusions using quantified uncertainty. Just remember that there is no magic formula that turns uncertainty into certainty, we cannot escape it!


Statistics has two main branches, descriptive statistics and inferential statistics, each tackling uncertainty differently. 
## Descriptive Statistics
This is like taking a selfie of your data. It summarizes and reports what‚Äôs there, showing us things like averages, spreads, and patterns. Think of it as data gossip ‚Äî who‚Äôs the biggest, smallest, most popular, or way off in the corner doing their own thing (*yes, outliers, I'm looking at you!*).

For example, a histogram or boxplot can reveal whether our data is skewed, normally distributed, or hiding some unexpected surprises. Without descriptive analysis, we risk making assumptions that could lead to misleading conclusions‚Äîkind of like blindly trusting a GPS without checking if the road exists!


## Inferential Statistics
This is where things get spicy! Instead of just describing what we see, we gain insight or make predictions about the big picture based on a small sample. It‚Äôs like tasting one donut from the box and guessing if the whole batch is good üç©.

Consider you have a hypothesis you wish to test (*alternative hypothesis*). Using data collected, you express the rules of the game through probabilities, distributions, and statistical models. This helps create a ‚Äútoy model‚Äù of the world based on your hypothesis.  The null hypothesis is then the ‚Äúdefault world,‚Äù and your working hypothesis is literally everything else. You pretend you know how things work, and then check if reality agrees with you (*hypothesis testing*). 
Then you ask the big question: *Does our data make the null hypothesis look completely ridiculous?* If yes, we might just reject it and revise the null world accordingly.



Statistics isn‚Äôt about finding the ultimate truth, but about making the best possible decisions with the information available. We estimate, test, and adjust‚Äîbecause in the end, being less wrong is the best we can do.
In the words of the famous statistician George Box: *"All models are wrong, but some are useful"*.

## This Book

The Statistical Inference Pipeline‚Äù

This diagram illustrates the statistical pipeline, a structured process for drawing conclusions from data:
	1.	Producing Data ‚Äì A sample is extracted from the population.
	2.	Descriptive Analysis ‚Äì The sample is summarized using descriptive statistics.
	3.	Probability Modeling ‚Äì Statistical models estimate the relationship between the sample and the population.
	4.	Inference ‚Äì Conclusions about the entire population are made based on the sample data.

This pipeline is fundamental in statistics, enabling researchers to make data-driven decisions with limited information.



‚ÄúThe Statistical Data Pipeline‚Äù

This diagram represents the pipeline of statistical analysis, outlining the key steps in drawing insights from data:
	1.	Producing Data ‚Äì A sample is selected from the population to represent the whole.
	2.	Descriptive Statistics ‚Äì The sample is analyzed to summarize patterns and trends.
	3.	Probability Modeling ‚Äì Statistical methods establish connections between the sample and population.
	4.	Inference ‚Äì Findings from the sample are generalized to make conclusions about the population.

This structured pipeline is essential in statistics, ensuring reliable and data-driven decision-making.
The analysis process is visualized in @fig-popsamp.


::: center
![The Statistical Data Pipeline](imgs/‚Äépopsamp-pipe.png){#fig-popsamp fig-align="center" width="100%"}
:::



We will cover each of the steps depicted in @fig-popsamp in this book. Step 1 refers to the data collection phase which is covered in chapter The sample is analyzed to summarize key characteristics.....
Step 2 is on the xploratory analysis of data and is covered in chapters....
Next is the rhid step concerning the relationship between sample data and the population is modeled. is covered in chaopters.. and finally, inferential statistic where we Conclusions are drawn about the population based on the sample.
---


