[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Foundational Statistics",
    "section": "",
    "text": "Welcome\nThis book is work in progress.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "use subdir/*.qmd in yaml",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "9  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Summary</span>"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "descript-stats.html",
    "href": "descript-stats.html",
    "title": "Descriptive Statistics",
    "section": "",
    "text": "Describing a data set\nGraphical Methods for Summarizing Data\nMeasures of Locations and Dispersion\nIndex\n\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Descriptive Statistics"
    ]
  },
  {
    "objectID": "prob-theory.html",
    "href": "prob-theory.html",
    "title": "3  Probability Theory",
    "section": "",
    "text": "Definition and Interpretation of probabilities\nSet theory\nCounting rules for probabilities (Complement rule, addition rule, Bayes, Independence, etc)\n\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Probability Theory</span>"
    ]
  },
  {
    "objectID": "rv-probdists.html",
    "href": "rv-probdists.html",
    "title": "4  Random Variables and Probability Distributions",
    "section": "",
    "text": "Discrete random variables and distributions\nContinuous random variables and distributons\nMultivariate Random Variables\n\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Random Variables and Probability Distributions</span>"
    ]
  },
  {
    "objectID": "sampling-dists.html",
    "href": "sampling-dists.html",
    "title": "5  Sampling and Sampling Distributions",
    "section": "",
    "text": "Sampling Distribution for a sample mean\nSampling Distribution for a sample proportion\nCentral Limit Theorem\n\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sampling and Sampling Distributions</span>"
    ]
  },
  {
    "objectID": "inference.html",
    "href": "inference.html",
    "title": "6  Inferential Statistics",
    "section": "",
    "text": "Estimation: point and interval estimation\nHypothesis testing for a sample mean and sample proportion\nHypothesis testing with confidence intervals\nHypothesis testing for the difference of two means or proportions\n\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Inferential Statistics</span>"
    ]
  },
  {
    "objectID": "chi2.html",
    "href": "chi2.html",
    "title": "7  Chi Square Tests",
    "section": "",
    "text": "1 + 1\n\n[1] 2",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Chi Square Tests</span>"
    ]
  },
  {
    "objectID": "regression.html",
    "href": "regression.html",
    "title": "Regression Analysis",
    "section": "",
    "text": "Simple Linear Regression\nMultiple Linear Regression\nOLS estimation\nGauss Markov Theorem\nModel Assumptions and Diagnostics\nDummy variables and Interaction Terms\nEndogeneity and IV estimation\n\n\n1 + 1\n\n[1] 2",
    "crumbs": [
      "Regression Analysis"
    ]
  },
  {
    "objectID": "slr.html",
    "href": "slr.html",
    "title": "2  Simple Linear Regression",
    "section": "",
    "text": "1 + 1\n\n[1] 2",
    "crumbs": [
      "Regression Analysis",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Simple Linear Regression</span>"
    ]
  },
  {
    "objectID": "intro-math.html",
    "href": "intro-math.html",
    "title": "1  The Unavoidable Math",
    "section": "",
    "text": "1.1 The Sum and The Product\nWe write the sum of \\(n\\) numbers denoted \\(x_1,x_2,\\ldots,x_n\\) as \\[\\sum_{i=1}^n x_i = x_1 +x_2 + \\cdots + x_n \\] This is read as the sum of \\(x_i\\) where \\(i\\) goes from 1 to \\(n\\). The letter \\(i\\) is called the summation index and can be chosen to be any other letter.\nSimilarly, the product of \\(n\\) numbers denoted \\(x_1,x_2,\\ldots,x_n\\) is written as \\[\\prod_{i=1}^n x_i = x_1 \\times x_2 \\times  \\cdots \\times x_n \\].",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Unavoidable Math</span>"
    ]
  },
  {
    "objectID": "intro-math.html#the-sum",
    "href": "intro-math.html#the-sum",
    "title": "1  A Little Bit of Math",
    "section": "",
    "text": "1.1.1 Example\nAssume 5 values on \\(x\\) denoted \\(x_1,x_2,x_3,x_4,x_5\\). How can we write the sum of the squared difference of each of these values to their mean value \\(\\overline{x}\\)? \\[(x_1-\\overline{x})^2 + (x_2-\\overline{x})^2 + (x_3-\\overline{x})^2 + (x_4-\\overline{x})^2 + (x_1-\\overline{x})^5 \\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\\sum_{i=1}^n (x_i-\\overline{x})^2\\]",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A Little Bit of Math</span>"
    ]
  },
  {
    "objectID": "intro-math.html#the-sum-and-the-product",
    "href": "intro-math.html#the-sum-and-the-product",
    "title": "1  The Unavoidable Math",
    "section": "",
    "text": "1.1.1 Example\nAssume 5 values on \\(x\\) denoted \\(x_1,x_2,x_3,x_4,x_5\\). How can we write the sum of the squared difference of each of these values to their mean value \\(\\overline{x}\\)? \\[(x_1-\\overline{x})^2 + (x_2-\\overline{x})^2 + (x_3-\\overline{x})^2 + (x_4-\\overline{x})^2 + (x_1-\\overline{x})^5 \\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\\sum_{i=1}^n (x_i-\\overline{x})^2\\]",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Unavoidable Math</span>"
    ]
  },
  {
    "objectID": "intro-math.html#combinatorics",
    "href": "intro-math.html#combinatorics",
    "title": "1  The Unavoidable Math",
    "section": "1.2 Combinatorics",
    "text": "1.2 Combinatorics\nThe next couple of mathematical concepts covered here are closely linked to the theory of probability which we will cover later in this book. Combinatorics is about counting the number of possibilities to do something. In how many ways?, is a common question here. The rule of product, also knows as the multiplication principle, is a basic counting principle (a.k.a. the fundamental principle of counting). Assume that you have to perform \\(k\\) tasks in turn (one after the other). The first task can be performed in \\(n_1\\) different ways, the second in \\(n_2\\) different ways, etc. In how many different ways can one perform the \\(k\\) tasks in turn? The number of possible ways to perform the \\(k\\) tasks in turn is given by \\[n_1 \\times n_2 \\times  \\cdots \\times n_k\\]\n\n1.2.1 Example\nAssume you are looking at a menu with 3 starters, 4 main courses and 2 desserts. In how many ways can a three-course meal be composed?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[3\\cdot   4 \\cdot 2 = 24\\]",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Unavoidable Math</span>"
    ]
  },
  {
    "objectID": "intro-math.html#the-multiplication-principle",
    "href": "intro-math.html#the-multiplication-principle",
    "title": "1  A Little Bit of Math",
    "section": "1.3 The Multiplication Principle",
    "text": "1.3 The Multiplication Principle\nAssume that you have to perform \\(k\\) tasks in turn (one after the other). The first task can be performed in \\(n_1\\) different ways, the second in \\(n_2\\) different ways, etc. In how many different ways can one perform the \\(k\\) tasks in turn? The number of possible ways to perform the \\(k\\) tasks in turn is given by \\[n_1 \\times n_2 \\times  \\cdots \\times n_k\\]\n\n1.3.1 Example\nAssume you are looking at a menu with 3 starters, 4 main courses and 2 desserts. In how many ways can a three-course meal be composed?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[3 \\times 4 \\times 2 = 24\\]\n\n1.4 Permutations",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>A Little Bit of Math</span>"
    ]
  },
  {
    "objectID": "intro-math.html#permutations",
    "href": "intro-math.html#permutations",
    "title": "1  The Unavoidable Math",
    "section": "1.3 Permutations",
    "text": "1.3 Permutations\nPermutations refers to the mathematical calculation of the number of ways a particular set can be arranged. An arrangement of \\(n\\) different objects in a specific order is called a permutation of the objects. The number of permutations that can be formed from \\(n\\) different objects is \\[n! = n\\cdot (n-1)\\cdot (n-2)\\cdot 2\\cdot 1 \\] This is read as \\(n\\)-factorial.\n\n\n\n\n\n\nNote\n\n\n\n\\[ 0!=1 \\]\n\n\n\n1.3.1 Example\nIn how many different ways can we permute the three objects \\(A\\), \\(B\\) and \\(C\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[3! = 3 \\cdot 2 \\cdot 1 = 6\\] namely: \\(ABC,\\  ACB, \\ BAC, \\ BCA,\\  CAB, \\ CBA\\)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Unavoidable Math</span>"
    ]
  },
  {
    "objectID": "variable-class.html",
    "href": "variable-class.html",
    "title": "3  Variable Classification",
    "section": "",
    "text": "3.1 Types of Variables\nTo facilitate analysis, we focus on measurable variables, which are broadly categorized into quantitative and qualitative types. There are further sub-classes for each of these main classes, as shown in Figure 3.1 and exemplified further in the following.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Variable Classification</span>"
    ]
  },
  {
    "objectID": "variable-class.html#definition-of-a-variable",
    "href": "variable-class.html#definition-of-a-variable",
    "title": "3  Variable Classification",
    "section": "",
    "text": "The choice of measures of central tendency and dispersion\nThe choice of correlation measures\nThe selection of appropriate graphical representations",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Variable Classification</span>"
    ]
  },
  {
    "objectID": "variable-class.html#types-of-variables",
    "href": "variable-class.html#types-of-variables",
    "title": "3  Variable Classification",
    "section": "",
    "text": "graph TD;\n    A(Variable) --&gt; B(Qualitative);\n    A(Variable) --&gt; C(Quantitative);\n    C --&gt; D(Discrete);\n    C --&gt; E(Continuous);\n\n\n\n\n\n\n\n\nFigure 3.1: Variable classification flowchart.\n\n\n\n\n3.1.1 Quantitative (Numerical)\nThese variables are represented by numbers and can be measured. Depending on the type of numbers a variable takes, it can be classified as discrete or continuous.\nDiscrete variables take specific, distinct values and cannot be subdivided (natural, integer, or rational numbers). Think of these as variables holding countable values. Examples include number of children in a family, number of goals ina football match, and number of sales transactions per day.\nContinuous variables can take any value within a given range of values within an interval and can be infinitely divided (real numbers). Examples include hegiht, weight, stock price and distance.\n\n\n3.1.2 Qualitative (Categorical) Variables\nThese variables represent data that can be divided into distinct groups or categories. These values do not have a natural numerical order (except for ordinal variables) and must be coded into numerical values for statistical analysis. Examples include political affiliation, blood type, movie genre and social media platform.\nA special case of qualitative variables that take only two possible values, so called dichotomous or binary variable. Examples include smoking status (Smoker/Non-smoker) and COVID-19 test result (positive/negative).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Variable Classification</span>"
    ]
  },
  {
    "objectID": "variable-class.html#levels-of-measurement",
    "href": "variable-class.html#levels-of-measurement",
    "title": "3  Variable Classification",
    "section": "3.2 Levels of Measurement",
    "text": "3.2 Levels of Measurement\nThe characteristics of collected data allow us to classify them into four different measurement levels. These levels determine the statistical operations that can be performed. Table 3.1 summarizes the different measurement levels described in the following.\nNominal scale categorizes data without any inherent order. For example, there is no inherent ordering in the variable eye color (blue, brown, green), gender or nationality. You can only distinguish between the values nominal variables take.\nOrdinal scale is when data can be ranked in a meaningful order, but differences between values are not necessarily equal or meaningful, for example when looking at the variable fruit preference ranking or customer satisfaction levels (low, medium, high). In other words, interval lengths between one variable value and another are not of the same length.\nInterval scale is similar to the ordinal scale but with equal intervals between values. However, it lacks a true zero point. Examples include Temperature in Celsius and calendar years (the year 2000 is 100 years after 1900, but the year 0 does not represent the “beginning of time”).\nFinally, ratio scale has all of the above properties but also a natural zero point, allowing meaningful calculations of differences and ratios. Examples here include salary, distance traveled, height, and weight.\n\n\n\nTable 3.1: Table Summary of Measurement Levels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistinguish\nRank\nEqual Step Length\nAbsolute Zero Point\nExample\n\n\n\n\nNominal\nYes\nNo\nNo\nNo\ngender, city, religion\n\n\nOrdinal\nYes\nYes\nNo\nNo\ngrades, preference, customer satisfaction ratings\n\n\nInterval\nYes\nYes\nYes\nNo\ntemperature in Celsius. credit scores, calender years\n\n\nRatio\nYes\nYes\nYes\nYes\nlength, weight, time, salary",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Variable Classification</span>"
    ]
  },
  {
    "objectID": "variable-class.html#types-of-numbers",
    "href": "variable-class.html#types-of-numbers",
    "title": "3  Variable Classification",
    "section": "3.3 Types of Numbers",
    "text": "3.3 Types of Numbers\nUnderstanding the nature of numbers is crucial when classifying variables because it directly influences statistical analysis, measurement accuracy, and the interpretation of data. The classification of numbers into natural, whole, integer, rational, irrational, and real numbers helps in determining which mathematical operations and statistical techniques are valid for a given data set:\n\nNatural Numbers: \\(0, 1, 2, 3, \\ldots\\)\nIntegers: \\(\\dots , -3, -2, -1, 0, 1, 2, 3, \\ldots\\)\nRational Numbers: Numbers that can be expressed as a fraction \\(\\frac{a}{b}\\) , where \\(a\\) and \\(b\\) are integers. Examples include:\n\n\\(-14 = \\frac{-14}{1}\\)\n\\(\\frac{3}{4} = 0.75\\)\n\\(\\frac{2}{7} = 0.285714285714 \\dots\\)\n\nReal Numbers: Non-repeating decimal numbers, such as:\n\n\\(\\pi = 3.14159265358979 \\dots\\)\n\n\nFor example you cannot calculate an average zip code (nominal) or say that “a temperature of 20°C is twice as hot as 10°C” (interval), but you can say “a person earning €50,000 earns twice as much as someone earning €25,000” (ratio). An another example, you cannot consider the mean of a categorical variable like “favorite color,” but you can analyze the frequency distribution.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Variable Classification</span>"
    ]
  },
  {
    "objectID": "variable-class.html#variable-classification-flowchart",
    "href": "variable-class.html#variable-classification-flowchart",
    "title": "3  Variable Classification",
    "section": "3.4 Variable Classification Flowchart",
    "text": "3.4 Variable Classification Flowchart\nThe following diagram illustrates the classification of variables into qualitative and quantitative types.\n\n\n\n\n\ngraph TD;\n    A(Variable) --&gt; B(Qualitative);\n    A(Variable) --&gt; C(Quantitative);\n    C --&gt; D(Discrete);\n    C --&gt; E(Continuous);\n\n\n\n\n\n\n\n3.4.1 Quantitative (Numerical)\nThese variables are represented by numbers and can be measured. Depending on the type of numbers a variable takes, it can be classified as discrete or continuous.\nDiscrete variables take specific, distinct values and cannot be subdivided (natural, integer, or rational numbers). Think of these as variables holding countable values. Examples include number of children in a family, number of goals ina football match, and number of sales transactions per day.\nContinuous variables can take any value within a given range of values within an interval and can be infinitely divided (real numbers). Examples include hegiht, weight, stock price and distance.\n\n\n3.4.2 Qualitative (Categorical) Variables\nThese variables represent data that can be divided into distinct groups or categories. These values do not have a natural numerical order (except for ordinal variables) and must be coded into numerical values for statistical analysis. Examples include political affiliation, smoking status (Smoker/Non-smoker), blood type, movie genre, COVID-19 test result and social media platform.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Variable Classification</span>"
    ]
  },
  {
    "objectID": "intro-surveys.html",
    "href": "intro-surveys.html",
    "title": "2  Surveys: Key Concepts",
    "section": "",
    "text": "2.1 Census vs. Sample Surveys\nThe population refers to the complete set of elements (individuals, objects, or units) relevant to the study. The definition of a population can vary based on the research objective and can be finite (e.g., employees in a company) or infinite (e.g., potential customers in a market). A sample is a selected subset of the population. (inlcude figure)\nA census (or total survey) involves collecting data from every individual in a given population. This is common in national population counts but is often impractical for other types of research due to cost and time constraints. Instead, most studies rely on sample surveys which aim to generalize findings from the sample to the entire population with reasonable accuracy. The size of the sample (sample size) plays a crucial role in determining the reliability of the conclusions drawn.\nOne way to understand sampling is through the urn metaphor (Figure 2.1): Imagine an urn filled with different-colored balls representing different individuals in a population. Drawing balls at random with/without replacement simulates the process of selecting a sample from a population, emphasizing the role of randomness in reducing bias.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Surveys: Key Concepts</span>"
    ]
  },
  {
    "objectID": "intro-surveys.html#characteristics-of-a-population",
    "href": "intro-surveys.html#characteristics-of-a-population",
    "title": "2  Surveys: Key Concepts",
    "section": "2.2 Characteristics of a Population",
    "text": "2.2 Characteristics of a Population\nEach individual in a population has measurable attributes, such as height, weight, income, or opinions. When measuring a particular characteristic, we can calculate various population metrics such as:\n\nMean (average), e.g., the average height of individuals in a population.\nProportion, e.g., the percentage of women in a population.\nTotal values, e.g., the total number of items owned by a group.\nCounts of specific attributes, e.g., the number of people with a particular qualification.\n\nThe main challenge in survey research is determining how accurately a sample represents the entire population. Statistical methods help estimate key characteristics of a population based on sample data.\nThese population characteristics are fixed parameters values which are usually unknown. Common parameters are the mean (\\(\\mu\\)) representing the true average of a characteristic in the population, or the proportion (\\(p\\)) representing the fraction of the population with a certain attribute.\nSince parameters cannot always be measured directly, they are estimated using sample statistics. For example, the sample mean (\\(\\bar{x}\\)) based on sampled observations \\(x_1,x_2,\\ldots, x_n\\) is used to estimate the population mean (\\(\\mu\\)). (include figure)\nTo distinguish between parameters and estimates, Greek letters are typically used for population parameters, while Latin letters are used for sample estimates. We will however avoid using already taken parameters, for example \\(\\pi\\) is not here used for population parameter as this already has a value assigned to it. In such cases we use the Latin letter to indicate the parameter and a ‘hat’ over the letter indicate the sample estimate (that is \\(\\hat{p}\\)).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Surveys: Key Concepts</span>"
    ]
  },
  {
    "objectID": "intro-surveys.html#types-of-surveys",
    "href": "intro-surveys.html#types-of-surveys",
    "title": "2  Surveys: Key Concepts",
    "section": "2.3 Types of Surveys",
    "text": "2.3 Types of Surveys\nThe method chosen for collecting data in a survey plays a crucial role in determining the accuracy and reliability of the results. Broadly, survey research can be classified into experimental and non-experimental approaches, each serving different research purposes.\n\n2.3.1 Experimental Surveys\n**Experimental surveys* are designed to explore causal relationships between variables.Here, researchers have control over certain conditions, manipulating one or more variables while keeping others constant to observe the effects. A key feature of experimental surveys is randomization, where participants are randomly assigned to different groups to eliminate bias. By ensuring that external factors do not influence the results, researchers can draw strong conclusions about cause and effect.\nOne common application of experimental surveys is in medical research, where clinical trials are conducted to test the effectiveness of a new drug. In such cases, patients might be randomly assigned to either a treatment group receiving the drug or a control group receiving a placebo. The outcomes are then compared to determine the drug’s efficacy. Similarly, in marketing, companies may experiment with different advertising strategies by exposing randomly selected groups to different promotional campaigns and then measuring their purchasing behavior.\nWhile experimental surveys provide strong evidence of causal relationships, they do have some limitations. They tend to be resource-intensive, requiring significant time and financial investment. Furthermore, ethical considerations may restrict certain types of experiments, especially in cases where withholding treatment or intervention from a control group could have serious consequences. Another challenge is that controlled settings may not fully capture real-world complexities, making it difficult to generalize findings beyond the experimental conditions.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Surveys: Key Concepts</span>"
    ]
  },
  {
    "objectID": "intro-surveys.html#types-of-surveys-data-collection-methods",
    "href": "intro-surveys.html#types-of-surveys-data-collection-methods",
    "title": "2  Surveys: Key Concepts",
    "section": "2.3 Types of Surveys: Data Collection Methods",
    "text": "2.3 Types of Surveys: Data Collection Methods\nThe method chosen for collecting data in a survey plays a crucial role in determining the accuracy and reliability of the results. Broadly, survey research can be classified into experimental and non-experimental approaches, each serving different research purposes.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Surveys: Key Concepts</span>"
    ]
  },
  {
    "objectID": "intro-surveys.html#non-experimental-surveys-observing-trends-and-patterns",
    "href": "intro-surveys.html#non-experimental-surveys-observing-trends-and-patterns",
    "title": "2  Surveys: Key Concepts",
    "section": "2.4 Non-Experimental Surveys: Observing Trends and Patterns",
    "text": "2.4 Non-Experimental Surveys: Observing Trends and Patterns\nUnlike experimental surveys, non-experimental surveys focus on observing and describing characteristics, trends, and relationships within a population without direct intervention. These surveys are widely used in fields such as social sciences, market research, and public policy analysis, where the goal is often to collect descriptive data rather than establish causality.\nOne of the most common types of non-experimental surveys is the cross-sectional survey, which captures data from a population at a single point in time. This method is frequently used in opinion polls, customer satisfaction studies, and demographic research. For example, a company might conduct a survey to understand consumer preferences for a new product just before its launch. Because cross-sectional surveys are quick and cost-effective, they are widely used.\nFor studies that require tracking changes over time, researchers may turn to longitudinal surveys, which collect data from the same subjects at multiple intervals. Longitudinal surveys are especially useful for understanding long-term trends, such as how consumer behavior evolves over the years or how health outcomes change in response to lifestyle choices. In a panel study, the same individuals are followed over time, whereas in a cohort study, a specific group—such as people born in a particular year—is tracked to observe changes as they age. These methods are valuable in policy research, where understanding the long-term effects of interventions, such as educational reforms or public health initiatives, is critical.\nSome non-experimental surveys rely on observational data, where researchers study behaviors and interactions without directly questioning participants. Observational studies often aim to identify associations and generate hypotheses for further research. This method is commonly used in consumer behavior research. For example, a supermarket might analyze shopping patterns by tracking how customers navigate store aisles without them being aware of the observation. Another example is web tracking which is a modern form of observational research where companies monitor users’ online activities to analyze behavior, predict preferences, and personalize experiences. Unlike traditional observational studies, which are typically conducted with ethical oversight and clear participant consent, the presented examples often operates in the background without users’ full awareness or control. Thus, while observational studies provide authentic behavioral insights, they raise ethical concerns about privacy and cannot establish direct cause-and-effect relationships.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Surveys: Key Concepts</span>"
    ]
  },
  {
    "objectID": "intro-surveys.html#census-vs.-sample-surveys",
    "href": "intro-surveys.html#census-vs.-sample-surveys",
    "title": "2  Surveys: Key Concepts",
    "section": "",
    "text": "Figure 2.1: The urn metaphor.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Surveys: Key Concepts</span>"
    ]
  },
  {
    "objectID": "describe-data.html",
    "href": "describe-data.html",
    "title": "4  Describing a Dataset",
    "section": "",
    "text": "4.1 Describing Qualitative Variables\nQualitative (categorical) variables represent data grouped into distinct categories, such as gender, marital status, or election participation. These variables are best summarized using frequency tables and graphs such as bar charts and pie charts.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing a Dataset</span>"
    ]
  },
  {
    "objectID": "describe-data.html#describing-qualitative-variables",
    "href": "describe-data.html#describing-qualitative-variables",
    "title": "4  Describing a Dataset",
    "section": "",
    "text": "4.1.1 Frequency Tables for Qualitative Data\nA frequency table lists the categories of a variable along with their corresponding counts (absolute frequency) and percentages (relative frequency). Assume we surveyed 300 people about their favorite hot beverage and found that 60% prefer coffee and 40% prefer tea. This preference distribution, also presented in the table below, will be used as a running example in the following.\n\n\n\nPreference\nCount (\\(f_i\\))\nPercentage (\\(f_i\\) in %)\n\n\n\n\nTea\n120\n40%\n\n\nCoffee\n180\n60%\n\n\nTotal\n300\n100%\n\n\n\nThe relative frequency is calculated as:\n\\[\\frac{120}{300} \\times 100 = 40\\%\\]\nand shown in the third column.\n\n\n4.1.2 Pie Chart\nPie charts are one of the most commonly used tools for representing categorical data in a simple, visual format. They break down a whole into proportional slices, making it easy to see relative differences between categories at a glance. Whether you’re comparing sales across different product categories, analyzing survey responses, or breaking down a budget, a well-made pie chart provides an intuitive way to present proportions.\nEach slice represents a category’s percentage of the total, with the entire pie equaling 100%. The size of each slice is determined by the proportion of the category it represents. For example, if 60% of survey respondents prefer coffee over tea, that category would take up 60% of the pie chart, or 60% of 360°, that is \\(0.60 × 360° = 216°\\) of the full circle. Pie charts are particularly effective when comparing a few distinct categories but lose clarity when too many slices are included.\n\n\n\n\n\n\n\n\n\nWhile a standard pie chart is a great way to visualize data, 3D pie charts are the dark side of data visualization. They may look fancy, but they distort proportions, making it difficult to accurately compare slice sizes. Due to the perspective effect, some slices appear larger or smaller than they actually are, leading to misleading interpretations. In short: if you want your data to be clear and not just flashy, stick to 2D pies - your audience will thank you.\n\n\n4.1.3 Bar Chart\nBar charts are a great way to visualize qualitative data, making it easy to compare different categories. Each category is represented by a bar, with the height corresponding to its frequency or percentage. For example, a bar chart would clearly display the difference between coffee and tea lovers, making it easy to interpret at a glance. Unlike pie charts, bar charts work well even when multiple categories are involved, ensuring your audience can quickly grasp the data - without any risk of 3D chart-induced confusion!\n\n\n\n\n\n\n\n\n\n\n\n4.1.4 Contingency Tables\nWhen we have observations on two qualitative variables, we can create two separate frequency tables. However, if we want to study the relationship between the two variables, we use a contingency table.\nA contingency table organizes paired observations, showing the frequency distribution across the two categorical variables.\nFor example, consider the following data where individuals are classified into two groups based on their marital status (Married (M) or Not Married (NM)) and their voting behavior (Voted (1) or Did Not Vote (0)). This results in four possible outcome combinations:\n\n(M, 0) - Married, Did Not Vote\n(M, 1) - Married, Voted\n(NM, 0) - Not Married, Did Not Vote\n(NM, 1) - Not Married, Voted\n\nAssume the first four observations are: (G, 0), (EG, 1), (G, 1), (G, 1) Which we create the following table over:\n\n\n\n\n0 (Did Not Vote)\n1 (Voted)\n\n\n\n\nM (Married)\n✔️\n✔️✔️\n\n\nNM (Not Married)\n\n✔️\n\n\n\nOnce all observations have been recorded, we can create the following contingency table:\n\n\n\n\nDid Not Vote\nVoted\nTotal\n\n\n\n\nMarried\n54\n1496\n1550\n\n\nNot Married\n85\n628\n713\n\n\nTotal\n139\n2124\n2263\n\n\n\nThis table displays the distribution of voting behavior by marital status, where we can analyze differences between the groups.\n\nMarginal Distributions\nA marginal distribution summarizes the totals for each row and column in a contingency table. This helps us understand the overall distribution of each variable separately.\nFor example, the absolute and relative marginal distributions for marital status is shown below:\n\n\n\nMarital Status\nCount\nPercentage (%)\n\n\n\n\nMarried\n1550\n68.5%\n\n\nNot Married\n713\n31.5%\n\n\nTotal\n2263\n100%\n\n\n\nSimilarly, the absolute and relative marginal distributions for voting behvaior is shown below:\n\n\n\nVoting Behavior\nCount\nPercentage (%)\n\n\n\n\nDid Not Vote\n139\n6.1%\n\n\nVoted\n2124\n93.9%\n\n\nTotal\n2263\n100%\n\n\n\nThese tables summarize how many people are in each category without considering the second variable.\n\n\nConditional Distributions\nTo compare voting behavior between married and non-married individuals, we calculate row percentages.\n\n\n\n\nDid Not Vote (%)\nVoted (%)\nTotal (%)\n\n\n\n\nMarried\n3.5%\n96.5%\n100%\n\n\nNot Married\n11.9%\n88.1%\n100%\n\n\n\nThis table shows the conditional distribution of voting behavior, given marital status.\n\nAmong married individuals, 1496 × 100 = 0.965= 96.5% voted while 54 × 100 = 0.035 =3.5% did not.\nAmong non-married individuals, 88.1% voted, while 11.9% did not.\n\nBy comparing these row percentages, we can see that married individuals were more likely to vote compared to non-married individuals. We have calculated the percentages horizontally but compare the percentage values in the vertical columns. We can also compute column percentages instead of row percentages if needed.\nTo determine whether a relationship exists between voting behavior and marital status, we compare conditional distributions. Since the voting percentages differ between married and non-married groups, we conclude that marital status influences voting behavior. If the two variables were independent, the percentages in the columns would be nearly the same. The fact that they differ suggests an association between the two variables.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing a Dataset</span>"
    ]
  },
  {
    "objectID": "describe-data.html#frequency-tables-for-qualitative-data",
    "href": "describe-data.html#frequency-tables-for-qualitative-data",
    "title": "4  Describing a Dataset",
    "section": "5.1 Frequency Tables for Qualitative Data",
    "text": "5.1 Frequency Tables for Qualitative Data\nA frequency table lists the categories of a variable along with their corresponding counts (absolute frequency) and percentages (relative frequency). An example is given in Table 5.1.\n\n\n\nTable 5.1: Example: Gender Distribution in a Sample of 300 People\n\n\n\n\n\nGender\nCount (\\(f_i\\))\nPercentage (\\(f_i\\) in %)\n\n\n\n\nMale\n120\n40%\n\n\nFemale\n180\n60%\n\n\nTotal\n300\n100%\n\n\n\n\n\n\nThe relative frequency is calculated as:\n\\[\\frac{120}{300} \\times 100 = 40\\%\\]\nand shown in the second column of ?tbl-frq.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing a Dataset</span>"
    ]
  },
  {
    "objectID": "describe-data.html#describing-quantitative-variables",
    "href": "describe-data.html#describing-quantitative-variables",
    "title": "4  Describing a Dataset",
    "section": "4.2 Describing Quantitative Variables",
    "text": "4.2 Describing Quantitative Variables\nUnlike qualitative data, which can be subjective and harder to categorize, quantitative data enables direct comparisons, making it easier to identify patterns, test hypotheses, and make data-driven decisions.\nQuantitative variables are numeric and can be either discrete (specific values, such as test scores) or continuous (measured on a scale, such as weight). These variables are best summarized using frequency tables, histograms, and cumulative distributions.\n\n4.2.1 Frequency and Cumulative Frequency Tables\n\nExample: Mathematics Grade\nWe begin by examining a discrete variable with a small number of observations. Assume the mathematics grades (ranging from 1-5) of 25 students are:\n5 4 1 4 4 3 2 3 3 3 4 2 3 1 3 3 5 4 2 2 2 4 3 5 3\nWhen data is presented in this way, it is referred to as ungrouped data. Let:\n\n\\(x_i\\) = observed values, where \\(i = 1, \\ldots , n\\)\n\\(f_i\\) = frequency of the \\(i\\)-th variable value, where \\(i = 1, 2, \\ldots, k\\).\n\nFor our example here, \\(n = 25\\) (total observations) and \\(k = 5\\) (five distinct values of the variable “Mathematics Grades”).\nLet’s summarize this data by counting how many we have in each grade category:\n\n\n\nGrade (\\(x_i\\))\nCount\nFrequency (\\(f_i\\))\n\n\n\n\n1\n✔️✔️\n2\n\n\n2\n✔️✔️✔️✔️✔️\n5\n\n\n3\n✔️✔️✔️✔️✔️✔️✔️✔️✔\n9\n\n\n4\n✔️✔️✔️✔️✔️✔️\n6\n\n\n5\n✔️✔️✔️\n3\n\n\nTotal\n\n25\n\n\n\nThe sum of all frequencies equals the total number of observations: [ _{i=1}^{k} f_i = n ]\nWe have created a frequency table by grouping the data into categories which can be visualized using a bar chart:\n\n\n\n\n\n\n\n\n\nThe cumulative frequency tells us how many observations are less than or equal to a given value.\n\n\n\n\n\n\n\n\nScore (\\(x_i\\))\nAbsolute Frequency (\\(f_i\\))\nCumulative Frequency (\\(F_i\\))\n\n\n\n\n1\n2\n2\n\n\n2\n5\n7\n\n\n3\n9\n16\n\n\n4\n6\n22\n\n\n5\n3\n25\n\n\n\nCumulative frequencies are often displayed using a cumulative step graph:",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing a Dataset</span>"
    ]
  },
  {
    "objectID": "describe-data.html#grouping-continuous-data",
    "href": "describe-data.html#grouping-continuous-data",
    "title": "4  Describing a Dataset",
    "section": "4.3 Grouping Continuous Data",
    "text": "4.3 Grouping Continuous Data\nWhen dealing with a continuous variable or a discrete variable with many values, it is common to create class intervals and then display frequencies in a frequency table or graph.\n\nExample: Candy Bar Weights\n\n\n\n\n\nWe have observed 40 candy bars of a specific brand and recorded their weighs which are given in the following in ascending order:\n20.5 20.7 20.8 21.0 21.0 21.4 21.5 22.0 22.1 22.5\n22.6 22.6 22.7 22.7 22.9 22.9 23.1 23.3 23.4 23.5\n23.6 23.6 23.6 23.9 24.1 24.3 24.5 24.5 24.8 24.8\n24.9 24.9 25.1 25.1 25.2 25.6 25.8 25.9 26.1 26.7\nSince weight is a continuous variable, we must group the observations into classes. We choose five class intervals, each with a width of 1.3 grams, starting from 20.4 grams:\n\nClass 1: 20.4 - 21.6\nClass 2: 21.7 - 22.9\nClass 3: 23.0 - 24.2\nClass 4: 24.3 - 25.5\nClass 5: 25.6 - 26.9\n\nWe can then create a frequency table as before:\n\n\n\nWeight Range (grams)\nFrequency (\\(f_i\\))\n\n\n\n\n20.4 - 21.6\n7\n\n\n21.7 - 22.9\n9\n\n\n23.0 - 24.2\n9\n\n\n24.3 - 25.5\n10\n\n\n25.6 - 26.9\n5\n\n\nTotal\n40\n\n\n\nWe then can visualize the frequency distribution using a histogram:\n\n\n\n\n\n\n\n\n\nGenerally, A histogram represents continuous data by grouping values into intervals, with bar heights corresponding to frequencies.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing a Dataset</span>"
    ]
  },
  {
    "objectID": "describe-data.html#conclusion",
    "href": "describe-data.html#conclusion",
    "title": "4  Describing a Dataset",
    "section": "4.5 Conclusion",
    "text": "4.5 Conclusion\nDescriptive statistics provides essential tools for summarizing and interpreting data. Qualitative variables are best visualized using frequency tables, pie charts, and bar charts, while quantitative data benefits from histograms, cumulative distributions, and grouped frequency tables. Understanding these methods allows for better data-driven decision-making, whether in research, business, or everyday life.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing a Dataset</span>"
    ]
  },
  {
    "objectID": "1.1-intro-math.html",
    "href": "1.1-intro-math.html",
    "title": "1  The Unavoidable Math",
    "section": "",
    "text": "1.1 The Sum and The Product\nWe write the sum of \\(n\\) numbers denoted \\(x_1,x_2,\\ldots,x_n\\) as \\[\\sum_{i=1}^n x_i = x_1 +x_2 + \\cdots + x_n \\] This is read as the sum of \\(x_i\\) where \\(i\\) goes from 1 to \\(n\\). The letter \\(i\\) is called the summation index and can be chosen to be any other letter.\nSimilarly, the product of \\(n\\) numbers denoted \\(x_1,x_2,\\ldots,x_n\\) is written as \\[\\prod_{i=1}^n x_i = x_1 \\times x_2 \\times  \\cdots \\times x_n \\].",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Unavoidable Math</span>"
    ]
  },
  {
    "objectID": "1.1-intro-math.html#the-sum-and-the-product",
    "href": "1.1-intro-math.html#the-sum-and-the-product",
    "title": "1  The Unavoidable Math",
    "section": "",
    "text": "1.1.1 Example\nAssume 5 values on \\(x\\) denoted \\(x_1,x_2,x_3,x_4,x_5\\). How can we write the sum of the squared difference of each of these values to their mean value \\(\\overline{x}\\)? \\[(x_1-\\overline{x})^2 + (x_2-\\overline{x})^2 + (x_3-\\overline{x})^2 + (x_4-\\overline{x})^2 + (x_1-\\overline{x})^5 \\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\\sum_{i=1}^n (x_i-\\overline{x})^2\\]",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Unavoidable Math</span>"
    ]
  },
  {
    "objectID": "1.1-intro-math.html#combinatorics",
    "href": "1.1-intro-math.html#combinatorics",
    "title": "1  The Unavoidable Math",
    "section": "1.2 Combinatorics",
    "text": "1.2 Combinatorics\nThe next couple of mathematical concepts covered here are closely linked to the theory of probability which we will cover later in this book. Combinatorics is about counting the number of possibilities to do something. In how many ways?, is a common question here. The rule of product, also knows as the multiplication principle, is a basic counting principle (a.k.a. the fundamental principle of counting). Assume that you have to perform \\(k\\) tasks in turn (one after the other). The first task can be performed in \\(n_1\\) different ways, the second in \\(n_2\\) different ways, etc. In how many different ways can one perform the \\(k\\) tasks in turn? The number of possible ways to perform the \\(k\\) tasks in turn is given by \\[n_1 \\times n_2 \\times  \\cdots \\times n_k\\]\n\n1.2.1 Example\nAssume you are looking at a menu with 3 starters, 4 main courses and 2 desserts. In how many ways can a three-course meal be composed?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[3\\cdot   4 \\cdot 2 = 24\\]",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Unavoidable Math</span>"
    ]
  },
  {
    "objectID": "1.1-intro-math.html#permutations",
    "href": "1.1-intro-math.html#permutations",
    "title": "1  The Unavoidable Math",
    "section": "1.3 Permutations",
    "text": "1.3 Permutations\nPermutations refers to the mathematical calculation of the number of ways a particular set can be arranged. An arrangement of \\(n\\) different objects in a specific order is called a permutation of the objects. The number of permutations that can be formed from \\(n\\) different objects is \\[n! = n\\cdot (n-1)\\cdot (n-2)\\cdot 2\\cdot 1 \\] This is read as \\(n\\)-factorial.\n\n\n\n\n\n\nNote\n\n\n\n\\[ 0!=1 \\]\n\n\n\n1.3.1 Example\nIn how many different ways can we permute the three objects \\(A\\), \\(B\\) and \\(C\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[3! = 3 \\cdot 2 \\cdot 1 = 6\\] namely: \\(ABC,\\  ACB, \\ BAC, \\ BCA,\\  CAB, \\ CBA\\)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>The Unavoidable Math</span>"
    ]
  },
  {
    "objectID": "describe-data.html#histograms",
    "href": "describe-data.html#histograms",
    "title": "4  Describing a Dataset",
    "section": "4.3 Histograms",
    "text": "4.3 Histograms\nWhen dealing with a continuous variable or a discrete variable with many values, it is common to create class intervals and then display frequencies in a frequency table or graph.\n\nExample: Candy Bar Weights\n\n\n\n\n\nWe have observed 40 candy bars of a specific brand and recorded their weighs which are given in the following in ascending order:\n20.5 20.7 20.8 21.0 21.0 21.4 21.5 22.0 22.1 22.5\n22.6 22.6 22.7 22.7 22.9 22.9 23.1 23.3 23.4 23.5\n23.6 23.6 23.6 23.9 24.1 24.3 24.5 24.5 24.8 24.8\n24.9 24.9 25.1 25.1 25.2 25.6 25.8 25.9 26.1 26.7\nSince weight is a continuous variable, we must group the observations into classes. We choose five class intervals, each with a width of 1.3 grams, starting from 20.4 grams:\n\nClass 1: 20.4 - 21.6\nClass 2: 21.7 - 22.9\nClass 3: 23.0 - 24.2\nClass 4: 24.3 - 25.5\nClass 5: 25.6 - 26.9\n\nWe can then create a frequency table as before:\n\n\n\nWeight Range (grams)\nFrequency (\\(f_i\\))\n\n\n\n\n20.4 - 21.6\n7\n\n\n21.7 - 22.9\n9\n\n\n23.0 - 24.2\n9\n\n\n24.3 - 25.5\n10\n\n\n25.6 - 26.9\n5\n\n\nTotal\n40\n\n\n\nWe then can visualize the frequency distribution using a histogram:\n\n\n\n\n\n\n\n\n\nGenerally, A histogram represents continuous data by grouping values into intervals, with bar heights corresponding to frequencies.\nTo determine how many observations fall below a given value, we calculate the cumulative frequencies as before and visualize using a step chart.\n\n\n\n\n\n\n\n\n\n\nWeight Range (grams)\nAbsolute Frequency (fi)\nCumulative Frequency (Fi)\nRelative Frequency (%)\nCumulative Relative Frequency (%)\n\n\n\n\n20.4 - 21.6\n7\n7\n17.5\n17.5\n\n\n21.7 - 22.9\n9\n16\n22.5\n40.0\n\n\n23.0 - 24.2\n9\n25\n22.5\n62.5\n\n\n24.3 - 25.5\n10\n35\n25.0\n87.5\n\n\n25.6 - 26.9\n5\n40\n12.5\n100\n\n\nTotal\n40\n40\n100%\n100%",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing a Dataset</span>"
    ]
  },
  {
    "objectID": "describe-data.html#stem-and-leaf-plot",
    "href": "describe-data.html#stem-and-leaf-plot",
    "title": "4  Describing a Dataset",
    "section": "4.4 Stem-and-Leaf Plot",
    "text": "4.4 Stem-and-Leaf Plot\nA Stem-and-leaf plot is a compact way to display numerical data while preserving individual values. It organizes data into stems (representing the leading digits) and leaves (the following digits), providing a good display of the distribution.\nFor example, in the dataset of candy bar weights, a stem-and-leaf plot can show whether weights cluster around a certain value and help identify any inconsistencies. This is shown in the follwing.\nWe split each value from our candy bar weight dataset into - Stem (e.g., 20, 21, 22, etc.) - Leaf (the decimal part, such as .1, .2, .3, etc.)\nThe Stem-and-Leaf Table is then given as:\n\n\n\nStem\nLeaf\n\n\n\n\n20\n5 7 8\n\n\n21\n0 0 4 5\n\n\n22\n0 1 5 6 6 7 7 9 9\n\n\n23\n1 3 4 5 6 6 6 9\n\n\n24\n1 3 5 5 8 8 9 9\n\n\n25\n1 1 2 6 8 9\n\n\n26\n1 7\n\n\n\nIf you tilt your head to the right or rotate the table 90° you get a fairly good view on the distribution of the data. The distribution appears fairly symmetric, with a slight skew toward the higher weights. Overall, the data is well distributed across the entire range, but there is a higher density of observations between 22.0 g and 24.9 g, indicating that the most frequent range appears to be 22 to 24 grams, with many values concentrated in these stems. The least frequent weights occur at the lower (20-21 g) and higher (25-26 g) ends. In a quality control one might check this distribution and note whether a large amount of candy bars end up in the tails of the distribution, thus indicating inconsistent production of candy bars.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Describing a Dataset</span>"
    ]
  },
  {
    "objectID": "central-measures.html",
    "href": "central-measures.html",
    "title": "5  Measures of Central Tendency",
    "section": "",
    "text": "Example: Income\nWe have income data (in thousands of €) for 18 individuals:\nWe will in the following show how we compute each of the shown measure of central tendency in the follwing.\nWe will see that for this example, the median income is the most representative value, as it is less affected by high-income extreme values.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "central-measures.html#arithmetic-mean",
    "href": "central-measures.html#arithmetic-mean",
    "title": "5  Measures of Central Tendency",
    "section": "5.1 Arithmetic Mean",
    "text": "5.1 Arithmetic Mean\nThe mean is calculated by summing all values and dividing by the number of observations. It is often used as a measure of central tendency because it incorporates all data points, making it a valuable summary statistic. However, the mean is sensitive to outliers, meaning that extreme values can pull it higher or lower, potentially misrepresenting the typical value in a skewed distribution. Despite this, in normally distributed data, the mean is a reliable and widely used indicator of the data set’s center.\nThe mean is calculated differently depending on whether we are working with an entire population or a sample: For a population, the mean (\\(\\mu\\)) is given by: \\[\n\\mu = \\frac{\\sum_{i=1}^{N} x_i}{N}\n\\]\nwhere \\(N\\) is the population size. For a sample, the mean (\\(\\bar{x}\\)) is an estimate of \\(\\mu\\) and is calculated as: \\[\n\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\\] where \\(n\\) is the sample size.\n\nExample: Income\nThe mean income is: \\[\\bar{x} = \\frac{20 + 22 + 24 + 24 + \\cdots + 85 + 90}{18} = 44.7 \\] Thus, the mean income is 44 700 €.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "central-measures.html#median",
    "href": "central-measures.html#median",
    "title": "5  Measures of Central Tendency",
    "section": "5.2 Median",
    "text": "5.2 Median\nThe median is the middle value of a dataset when arranged in ascending order. It represents the point where half of the observations are below and half are above, making it a useful measure of central tendency for skewed distributions. Unlike the mean, the median is not affected by outliers, making it a more robust indicator of typical values in cases where extreme values exist. For example, in income data, the median often provides a better reflection of the typical salary than the mean, which can be skewed by very high incomes. If there is an even number of observations, the median is the average of the two middle values. The median position is found using: \\[\n\\frac{n + 1}{2}\n\\]\n\nExample: Income\nFor our sorted income data we get that the median is located at the position \\[\n\\frac{19}{2} = 9.5\n\\] The median is then the average of the 9th and 10th values (40 and 44):** \\[\n\\frac{40 + 44}{2} =42\n\\] Thus, the median income is 42 000 €.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "central-measures.html#mode",
    "href": "central-measures.html#mode",
    "title": "5  Measures of Central Tendency",
    "section": "5.3 Mode",
    "text": "5.3 Mode\nThe mode is the most frequently occurring value in a data set. A data set can have one mode (unimodal), multiple modes (multimodal), or no mode at all if all values are unique. The mode is particularly useful for categorical data, such as identifying the most popular product in a sales dataset or the most common salary range in a workforce. In a histogram, the mode corresponds to the peak of the distribution, highlighting where data points concentrate the most.\n\nExample: Income\nThe most frequently occurring values in the income example is 24.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "central-measures.html#choosing-the-right-measure-of-central-tendency",
    "href": "central-measures.html#choosing-the-right-measure-of-central-tendency",
    "title": "5  Measures of Central Tendency",
    "section": "5.4 Choosing the Right Measure of Central Tendency",
    "text": "5.4 Choosing the Right Measure of Central Tendency\nThe appropriate measure of central tendency depends on the data type:\n\n\n\nData Type\nSuitable Measure(s)\n\n\n\n\nNominal Data (categories)\nMode\n\n\nOrdinal Data (ranked categories)\nMode, Median\n\n\nInterval Data (e.g., temperature)\nMode, Median, Mean\n\n\nRatio Data (e.g., income, weight)\nMode, Median, Mean\n\n\n\n\n5.4.1 When to Use Mean vs. Median\n\nUse the mean when data is normally distributed (without outliers).\nUse the median for skewed distributions or when outliers are present, as it is not affected by extreme values.\n\n\n\n5.4.2 Example in This Case\nIn our dataset, the median income (255,000 kr) is a better representation of a “typical” income than the mean (291,389 kr) because the presence of a few high-income values skews the mean upward.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "central-measures.html#conclusion",
    "href": "central-measures.html#conclusion",
    "title": "5  Measures of Central Tendency",
    "section": "5.5 Conclusion",
    "text": "5.5 Conclusion\n\nMean provides a good summary for normally distributed data but is sensitive to outliers.\nMedian is more robust in skewed distributions.\nMode is useful for categorical and multi-modal data but may not always provide useful insights.\n\nIn this dataset, the median income is the most representative value, as it is less affected by high-income outliers. Understanding central measures allows us to better interpret data distributions and make informed business and economic decisions. 📊",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "central-measures.html#which-measure-shoul-you-choose",
    "href": "central-measures.html#which-measure-shoul-you-choose",
    "title": "5  Measures of Central Tendency",
    "section": "5.4 Which Measure Shoul You Choose?",
    "text": "5.4 Which Measure Shoul You Choose?\nThe appropriate measure of central tendency depends on the data type:\n\n\n\nData Type\nSuitable Measure(s)\n\n\n\n\nNominal Data (categories)\nMode\n\n\nOrdinal Data (ranked categories)\nMode, Median\n\n\nInterval Data (e.g., temperature)\nMode, Median, Mean\n\n\nRatio Data (e.g., income, weight)\nMode, Median, Mean\n\n\n\n\n5.4.1 When to Use Mean vs. Median\n\nUse the mean when data is normally distributed (without outliers).\nUse the median for skewed distributions or when outliers are present, as it is not affected by extreme values.\n\n\n\n5.4.2 Example in This Case\nIn our dataset, the median income (255,000 kr) is a better representation of a “typical” income than the mean (291,389 kr) because the presence of a few high-income values skews the mean upward.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "central-measures.html#which-measure-should-you-choose",
    "href": "central-measures.html#which-measure-should-you-choose",
    "title": "5  Measures of Central Tendency",
    "section": "5.4 Which Measure Should You Choose?",
    "text": "5.4 Which Measure Should You Choose?\nThe appropriate measure of central tendency depends on the data type:\n\n\n\nData Type\nSuitable Measure(s)\n\n\n\n\nNominal Data (categories)\nMode\n\n\nOrdinal Data (ranked categories)\nMode, Median\n\n\nInterval Data (e.g., temperature)\nMode, Median, Mean\n\n\nRatio Data (e.g., income, weight)\nMode, Median, Mean\n\n\n\nWhen deciding between the mean and median, the mean is preferred for normally distributed data without outliers, while the median is better suited for skewed distributions or data sets with extreme values since it is not influenced by outliers. The mode, while useful for categorical and multimodal data, may not always provide meaningful insights in numerical data sets.\nIn our example, the median income (42 thousand euros) is a better representation of a “typical” income than the mean (44.7 thousand euros) due to high-income values skewing the mean upward.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "dispersion-measures.html",
    "href": "dispersion-measures.html",
    "title": "6  Measures of Dispersion",
    "section": "",
    "text": "Example: Test Scores\nConsider two sets of scores from two different groups of students. Each data set contains eight observations, representing the scores students received on a test:\nData A\nData B\nBoth data sets have the same mean: \\[\\bar{x}_A = \\frac{4 + 4 + 5 + 5 + 5 + 6 + 6 + 7}{8} = 5.25\\] \\[\\bar{x}_B = \\frac{0+1+4+5+6+7+8+11}{8} = 5.25\\]\nHowever, data set B has a much wider spread of values, ranging from 0 to 11, while data set A is more compact, with values between 4 and 7. The greater spread in data set B suggests higher variability in scores, meaning individual performances were less consistent compared to data set A. In contrast, data set A shows more uniform performance, suggesting students’ scores were relatively close to each other. The two data set distributions are visualized below:\nSeveral statistical measures help quantify dispersion in a dataset, some of which are covered in the following.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion-measures.html#quartiles-and-percentiles",
    "href": "dispersion-measures.html#quartiles-and-percentiles",
    "title": "6  Measures of Dispersion",
    "section": "6.1 Quartiles and Percentiles",
    "text": "6.1 Quartiles and Percentiles\nQuartiles and percentiles divide data into sections, helping us understand the distribution more effectively. The most commonly used quartiles are the first quartile (Q1), median (Q2), and third quartile (Q3).\n\nFirst Quartile (Q1) - 25th Percentile\nThe first quartile (Q1) marks the value below which 25% of the observations fall. It helps us understand the lower range of the dataset and is computed as: \\[\nQ1 = \\text{value at position } 0.25(n+1)\n\\] where \\(n\\) is the total number of observations.\n\n\nSecond Quartile (Q2) – 50th Percentile (Median)\nThe second quartile (Q2) is simply the median, dividing the dataset into two equal halves. This is calculated as: \\[\nQ2 = \\text{value at position } 0.50(n+1)\n\\] Since 50% of values are below this point, the median represents the central value in the distribution.\n\n\nThird Quartile (Q3) - 75th Percentile\nThe third quartile (Q3) is the value below which 75% of the observations fall. This is particularly useful for understanding the upper range of the dataset and is calculated as: \\[\nQ3 = \\text{value at position } 0.75(n+1)\n\\]\nQuartiles provide valuable information about how data is spread across different sections. They allow us to:\n\nIdentify skewness: If Q1 and Q3 are unevenly spaced around Q2 (the median), the data may be skewed.\nDetect outliers: Any value that is significantly lower than Q1 or higher than Q3 can be considered an outlier.\nCalculate the Interquartile Range (\\(IQR\\)), which is the difference between Q3 and Q1, providing a robust measure of spread that is less sensitive to extreme values (not to be confused with range which is the difference between minimum and maximum observation values \\(x_{max}-x_{min}\\)).\nCalculate the Quartile Deviation which is another measure that is robust against extreme values and defined as half the difference between the third quartile (Q3) and the first quartile (Q1): \\[\\frac{Q3-Q1}{2} .\\]",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion-measures.html#five-number-summary-adn-boxplot",
    "href": "dispersion-measures.html#five-number-summary-adn-boxplot",
    "title": "6  Measures of Dispersion",
    "section": "6.2 Five Number Summary adn Boxplot",
    "text": "6.2 Five Number Summary adn Boxplot",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion-measures.html#five-number-summary-and-boxplot",
    "href": "dispersion-measures.html#five-number-summary-and-boxplot",
    "title": "6  Measures of Dispersion",
    "section": "6.2 Five-Number Summary and Boxplot",
    "text": "6.2 Five-Number Summary and Boxplot\nA Five-Number Summary is a set of five descriptive statistics that provide insights into the distribution of a data set. These include:\n\nMinimum – The smallest observed value.\nFirst Quartile (Q1) – The 25th percentile, below which 25% of the data falls.\nSecond Quartile (Median, Q2) – The 50th percentile, the middle value of the data set.\nThird Quartile (Q3) – The 75th percentile, below which 75% of the data falls.\nMaximum – The largest observed value.\n\nThe Five-Number Summary helps in constructing a boxplot, which visually represents the spread and skewness of the data, as well as potential outliers. An example is shown in Figure 6.1.\nThe boxplot visually represents the distribution and spread of the data using the five-number summary. The minimum and maximum values mark the range of the data, while the first quartile (Q1), median (Q2), and third quartile (Q3) divide the data into four equal parts. The interquartile range (IQR), which spans from Q1 to Q3, highlights the middle 50% of the data, giving insights into variability.\nThe median (Q2) represents the central value, while outliers (if any) are shown as red points beyond the whiskers of the box. This boxplot effectively summarizes the data set, making it easy to identify skewness, dispersion, and potential outliers at a glance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.1: Boxplot with labels for each component of the Five-Number Summary\n\n\n\n\nExample: Test Scores\nWe compute the Five-Number Summary for the two datasets, A and B, representing test scores.\nData A\n4 4 5 5 5 6 6 7\nData B\n0 1 4 5 6 7 8 11\n1 and 5: The range is the difference between the maximum and minimum values (thus allowig us to see the minimum and maximum as well):\n\nA: (7 - 4 = 3)\nB: (11 - 0 = 11)\n\n2: The first quartile is found at position:\n\\[\nQ1 = 0.25(n+1) = 0.25(9) = 2.25\n\\]\n\nA: \\(Q1 = 4 + 0.25(5-4) = 4.25\\)\nB: \\(Q1 = 1 + 0.25(4-1) = 1.75\\)\n\n3: The median is found at position:\n\\[\nQ2 = 0.50(n+1) = 0.50(9) = 4.5\n\\]\n\nA: \\(Q2 = 5\\)\nB: \\(Q2 = 5.5\\)\n\n4: The third quartile is found at position:\n\\[\nQ3 = 0.75(n+1) = 0.75(9) = 6.75\n\\]\n\nA: \\(Q3 = 6 + 0.75(6-6) = 6\\)\nB: \\(Q3 = 7 + 0.75(8-7) = 7.75\\)\n\nFinal Five-Number Summaries\n\n\n\nDataset\nMinimum\nQ1\nMedian (Q2)\nQ3\nMaximum\n\n\n\n\nA\n4\n4.25\n5\n6\n7\n\n\nB\n0\n1.75\n5.5\n7.75\n11\n\n\n\nTo better understand the distribution of the two datasets, we use a boxplot to visualize the Five-Number Summary.\n\n\n\n\n\n\n\n\n\nThe boxplot visually highlights key aspects of dispersion, skewness, and potential outliers. In our example:\n\nDataset A has a smaller range (3) and is more compact.\nDataset B has a wider range (11), indicating greater variability in scores.\n\nBy using these descriptive statistics, we can better interpret datasets and make informed comparisons in various fields, including education, business, and research.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion-measures.html#conclusion",
    "href": "dispersion-measures.html#conclusion",
    "title": "6  Measures of Dispersion",
    "section": "6.7 Conclusion",
    "text": "6.7 Conclusion\nCovariance and correlation are essential tools for understanding the relationship between two variables. Covariance tells us about the direction of the relationship, while correlation standardizes this measure to provide a more intuitive interpretation. These concepts play a key role in statistical modeling, finance, and data analysis, helping us make informed decisions based on patterns in data. 📊",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion-measures.html#variance-adn-standard-deviation",
    "href": "dispersion-measures.html#variance-adn-standard-deviation",
    "title": "6  Measures of Dispersion",
    "section": "6.3 Variance adn Standard Deviation",
    "text": "6.3 Variance adn Standard Deviation",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion-measures.html#variance-and-standard-deviation",
    "href": "dispersion-measures.html#variance-and-standard-deviation",
    "title": "6  Measures of Dispersion",
    "section": "6.3 Variance and Standard Deviation",
    "text": "6.3 Variance and Standard Deviation\nWhen analyzing data, calculating the mean provides insight into the average value of a dataset. However, to understand how spread out the data is, we need to measure its variability. This is where variance and standard deviation become essential.\nVariance quantifies the average squared deviation of each data point from the mean. It provides a measure of how much the data points differ from the central value. For an entire population, the variance (\\(\\sigma^2\\)) is calculated as: \\[\nσ^2 = \\frac{\\sum_{i=1}^{N} (x_i - \\mu)^2}{N}\n\\] where:\n\n\\(N\\) = total number of data points in the population,\n\\(x_i\\) = individual data points,\n\\(\\mu\\) = population mean.\n\nWhen working with a sample instead of an entire population, we use the sample variance (\\(s^2\\)): \\[\ns^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\\] where:\n\n\\(n\\) = sample size,\n\\(x_i\\) = individual data points,\n\\(\\bar{x}\\)= sample mean.\n\nThe denominator \\((n-1)\\) instead of \\(n\\) accounts for the loss of one degree of freedom, making it an unbiased estimator of population variance (we’ll return to this later).\nAn alternative formula for calculating sample variance can be found by noting that the sum of all deviations from the mean is zero: \\(\\sum_{i=1}^{n} x_i = n\\bar{x}\\) so that we get \\[\n\\sum_{i=1}^{n} x_i \\bar{x} = \\bar{x} \\sum_{i=1}^{n} x_i = n \\bar{x}^2.\n\\] Substituting this back into original equation yields: \\[\ns^2 = \\frac{\\sum_{i=1}^{n} x_i^2 - 2n\\bar{x}^2 + n\\bar{x}^2}{n-1}\n= \\frac{\\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2}{n-1}\n\\] Rewriting using summation notation: \\[\ns^2 = \\frac{n \\sum_{i=1}^{n} x_i^2 - (\\sum_{i=1}^{n} x_i)^2}{n(n-1)}\n\\] This formulation simplifies calculations by hand when working with moderately large data sets.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion-measures.html#standard-deviation",
    "href": "dispersion-measures.html#standard-deviation",
    "title": "6  Measures of Dispersion",
    "section": "6.4 Standard Deviation",
    "text": "6.4 Standard Deviation\nThe standard deviation is the square root of variance, bringing it back to the same units as the data, for example if measuring weight in kg, standard deviation is in kg as well. Thus, it is better to use when you need an intuitive, practical measure of data spread in real-world scenarios.\nFor a population, the standard deviation (\\(\\sigma\\)) is: \\[\n\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{N} (x_i - \\mu)^2}{N}}\n\\] For a sample, the standard deviation (\\(s\\)) is: \\[\ns = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}}\n\\] which can also be rewritten as: \\[\ns = \\sqrt{\\frac{n \\sum_{i=1}^{n} x_i^2 - (\\sum_{i=1}^{n} x_i)^2}{n(n-1)}}\n\\] following the alternative variance formula shown above.\nTo better understand the concept of standard deviation, we visualize the distribution of a data set in Figure 6.2 where the mean (blue) and standard deviation bands (red) are overlayed.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.2: Histogram of data set of 100 0bservations with standard deviation bands included.\n\n\n\n\nExample: Test Scores\nReturning to our example on test scores, we previously calculated the sample mean as \\(\\bar{x} = 5.25\\). Now, we compute the variance (\\(s^2\\)) for each data set: \\[\ns^2_A = \\frac{(4−5.25)^2 + (4−5.25)^2 + \\dots + (7−5.25)^2}{8-1}  \\approx 1.074\n\\] \\[\ns^2_B = \\frac{(0−5.25)^2 + (1−5.25)^2 + \\dots + (11−5.25)^2}{8-1} \\approx 13.071\n\\]\nThe standard deviation (\\(s\\)) is then simply the square root of the variance: \\[\ns_A = \\sqrt{1.071} \\approx 1.035\n\\] and \\[\ns_B = \\sqrt{13.071} \\approx 3.615\n\\] We see that for data set A, the standard deviation is 1.035, indicating that most scores are relatively close to the mean (5.25), while for data set B, the standard deviation is 3.615, suggesting a wider spread of scores and greater variability. This comparison shows that data set B has a significantly higher variability than data set A, meaning the scores are more dispersed from the average.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion-measures.html#visualization-understanding-standard-deviation-with-a-histogram",
    "href": "dispersion-measures.html#visualization-understanding-standard-deviation-with-a-histogram",
    "title": "6  Measures of Dispersion",
    "section": "6.5 Visualization: Understanding Standard Deviation with a Histogram",
    "text": "6.5 Visualization: Understanding Standard Deviation with a Histogram\nTo better understand the concept of standard deviation, we visualize the distribution of a dataset and overlay the mean and standard deviation bands.\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning in geom_vline(aes(xintercept = mean_value), color = \"red\", linetype =\n\"dashed\", : Ignoring unknown parameters: `label`\n\n\nWarning in geom_vline(aes(xintercept = mean_value + sd_value), color = \"green\",\n: Ignoring unknown parameters: `label`\n\n\nWarning in geom_vline(aes(xintercept = mean_value - sd_value), color = \"green\",\n: Ignoring unknown parameters: `label`",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion-measures.html#the-empirical-rule",
    "href": "dispersion-measures.html#the-empirical-rule",
    "title": "6  Measures of Dispersion",
    "section": "6.5 The Empirical Rule",
    "text": "6.5 The Empirical Rule\nThe Empirical Rule, also known as the 68-95-99.7 Rule, describes how data is distributed in a normal (bell-shaped) distribution. It states that for a large population following a normal distribution:\n\nApproximately 68% of all observations lie within one standard deviation from the mean (\\(\\mu ± 1\\sigma\\)).\nApproximately 95% of all observations lie within two standard deviations from the mean (\\(\\mu ± 2\\sigma\\)).\nNearly all observations (99.7%) lie within three standard deviations from the mean (\\(\\mu ± 3\\sigma\\)).\n\nThis rule helps us understand the probability of an observation falling within a given range and is widely used in quality control, finance, and science to assess variability and expected outcomes. It is particularly useful when analyzing data distributions. If data follows a normal distribution most values cluster around the mean, and extreme values are rare. This also means that outliers can be identified if they fall beyond 3 standard deviations from the mean.\nThe empricial rule is visualized in Figure 6.3 showing the percentages of data falling within each standard deviation range.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.3: The Empirical Rule: Normal Distribution.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion-measures.html#covariance-and-correlation",
    "href": "dispersion-measures.html#covariance-and-correlation",
    "title": "6  Measures of Dispersion",
    "section": "6.6 Covariance and Correlation",
    "text": "6.6 Covariance and Correlation\nWhen analyzing data, it is often important to understand the relationship between two variables. Measures such as covariance and correlation help quantify the degree to which two variables change together, allowing us to assess their association.\nCovariance measures the direction of the linear relationship between two variables, \\(X\\) and \\(Y\\). It tells us whether an increase in one variable is associated with an increase or decrease in the other. For an entire population, the covariance is calculated as: \\[\nCov(X, Y) = \\sigma_{xy} = \\frac{\\sum_{i=1}^{N} (x_i - \\mu_x)(y_i - \\mu_y)}{N}\n\\] where:\n\n\\(N\\) = total number of observations,\n\\(x_i, y_i\\) = individual data points,\n\\(\\mu_x \\mu_y\\) = means of \\(X\\) and \\(Y\\).\n\nFor a sample, we estimate covariance using: \\[\nCov(X, Y) = s_{xy} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\n\\] where:\n\n\\(n\\) = sample size,\n\\(\\bar{x} ,\\bar{y}\\) = sample means of \\(X\\) and \\(Y\\).\n\nHow do we interpret the covariance?\n\nIf we have positive covariance it means that when \\(X\\) increases, then \\(Y\\) also tends to increase (e.g., study time and exam scores).\nIf we have negative covariance it means that when \\(X\\) increases, then \\(Y\\) tends to decrease (e.g., speed and time taken to reach a destination).\nIf we have near zero covariance, then this indicates no significant linear relationship between \\(X\\) and \\(Y\\) (note however that it does not detect patterns where variables are related in a non-linear way e.g., quadratic or exponential relationships).\n\nOne limitation of covariance is that it depends on the units of measurement (same as for variance), making it difficult to interpret. This is where correlation comes in as it standardizes covariance by adjusting for the scales of the variables, providing a dimensionless measure that is easier to interpret. The population correlation (\\(\\rho\\)) is given by \\[\n\\rho = \\frac{Cov(X, Y)}{\\sigma_x \\sigma_y}\n\\] where \\(\\sigma_x,\\sigma_y\\) are the standard deviations of \\(X\\) and \\(Y\\). The sample correlation (\\(r\\)) \\[\nr = \\frac{Cov(X, Y)}{s_x s_y}\n\\] where \\(s_x, s_y\\) are the sample standard deviations. Correlation values fall within the range -1 to 1, with the following interpretations (we use sample correlation as example):\n\n\\(r = 1\\): perfect positive correlation; \\(X\\) and \\(Y\\) move together exactly in a straight line.\n\\(0.8 \\leq r &lt; 1\\): strong positive correlation; \\(X\\) and \\(Y\\) tend to increase together.\n\\(0.5 \\leq r &lt; 0.8\\): moderate positive correlation; \\(X\\) and \\(Y\\) show a noticeable increasing relationship.\n\\(0 &lt; r &lt; 0.5\\): weak positive correlation; \\(X\\) and \\(Y\\) tend to increase together, but with variability.\n\\(r = 0\\): no linear relationship; \\(X\\) and \\(Y\\) are not linearly related (but might be non-linearly associated).\n\\(-0.5 &lt; r &lt; 0\\): weak negative correlation; as \\(X\\) increases, \\(Y\\) tends to decrease slightly.\n\\(-0.8 &lt; r \\leq -0.5\\): moderate negative correlation; \\(X\\) and \\(Y\\) shown an inverse relationship.\n\\(-1 ≤ r ≤ -0.8\\): strong negative correlation; \\(X\\) and \\(Y\\) move in opposite directions strongly.\n\\(r = -1\\): perfect negative correlation; \\(X\\) and \\(Y\\) move in exactly opposite directions in a straight line.\n\nA few examples are shown in Figure 6.4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6.4: Simulated data showing different correlations.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion-measures.html#correlation",
    "href": "dispersion-measures.html#correlation",
    "title": "6  Measures of Dispersion",
    "section": "6.7 Correlation",
    "text": "6.7 Correlation\nCorrelation standardizes covariance by adjusting for the scales of the variables, providing a dimensionless measure that is easier to interpret.\n\n6.7.1 Formula for Correlation\n\n6.7.1.1 Population Correlation (ρ)\n[ ρ = ] where σₓ, σᵧ are the standard deviations of X and Y.\n\n\n6.7.1.2 Sample Correlation (r)\n[ r = ] where sₓ, sᵧ are the sample standard deviations.\n\n\n\n6.7.2 Interpretation of Correlation\n\nr = 1 → Perfect positive correlation: X and Y move together exactly.\nr = -1 → Perfect negative correlation: X and Y move in exactly opposite directions.\nr = 0 → No linear relationship.\n|r| &lt; 0.5 → Weak correlation.\n0.5 ≤ |r| &lt; 0.8 → Moderate correlation.\n|r| ≥ 0.8 → Strong correlation.\n\nCorrelation is widely used in finance, economics, social sciences, and machine learning to measure relationships between variables, such as stock prices and market indices or advertising spending and sales.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "dispersion-measures.html#visualization-covariance-and-correlation-in-a-scatter-plot",
    "href": "dispersion-measures.html#visualization-covariance-and-correlation-in-a-scatter-plot",
    "title": "6  Measures of Dispersion",
    "section": "6.7 Visualization: Covariance and Correlation in a Scatter Plot",
    "text": "6.7 Visualization: Covariance and Correlation in a Scatter Plot\nTo better understand covariance and correlation, we visualize them using scatter plots.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/central-measures.html",
    "href": "descriptive-stats/central-measures.html",
    "title": "\n6  Measures of Central Tendency\n",
    "section": "",
    "text": "Example: Income\nWe have income data (in thousands of €) for 18 individuals:\nWe will in the following show how we compute each of the shown measure of central tendency in the follwing.\nWe will see that for this example, the median income is the most representative value, as it is less affected by high-income extreme values.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/central-measures.html#arithmetic-mean",
    "href": "descriptive-stats/central-measures.html#arithmetic-mean",
    "title": "\n6  Measures of Central Tendency\n",
    "section": "\n6.1 Arithmetic Mean",
    "text": "6.1 Arithmetic Mean\nThe mean is calculated by summing all values and dividing by the number of observations. It is often used as a measure of central tendency because it incorporates all data points, making it a valuable summary statistic. However, the mean is sensitive to outliers, meaning that extreme values can pull it higher or lower, potentially misrepresenting the typical value in a skewed distribution. Despite this, in normally distributed data, the mean is a reliable and widely used indicator of the data set’s center.\nThe mean is calculated differently depending on whether we are working with an entire population or a sample: For a population, the mean (\\(\\mu\\)) is given by: \\[\n\\mu = \\frac{\\sum_{i=1}^{N} x_i}{N}\n\\]\nwhere \\(N\\) is the population size. For a sample, the mean (\\(\\bar{x}\\)) is an estimate of \\(\\mu\\) and is calculated as: \\[\n\\bar{x} = \\frac{\\sum_{i=1}^{n} x_i}{n}\n\\] where \\(n\\) is the sample size.\nExample: Income\nThe mean income is: \\[\\bar{x} = \\frac{20 + 22 + 24 + 24 + \\cdots + 85 + 90}{18} = 44.7 \\] Thus, the mean income is 44 700 €.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/central-measures.html#median",
    "href": "descriptive-stats/central-measures.html#median",
    "title": "\n6  Measures of Central Tendency\n",
    "section": "\n6.2 Median",
    "text": "6.2 Median\nThe median is the middle value of a dataset when arranged in ascending order. It represents the point where half of the observations are below and half are above, making it a useful measure of central tendency for skewed distributions. Unlike the mean, the median is not affected by outliers, making it a more robust indicator of typical values in cases where extreme values exist. For example, in income data, the median often provides a better reflection of the typical salary than the mean, which can be skewed by very high incomes. If there is an even number of observations, the median is the average of the two middle values. The median position is found using: \\[\n\\frac{n + 1}{2}\n\\]\nExample: Income\nFor our sorted income data we get that the median is located at the position \\[\n\\frac{19}{2} = 9.5\n\\] The median is then the average of the 9th and 10th values (40 and 44):** \\[\n\\frac{40 + 44}{2} =42\n\\] Thus, the median income is 42 000 €.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/central-measures.html#mode",
    "href": "descriptive-stats/central-measures.html#mode",
    "title": "\n6  Measures of Central Tendency\n",
    "section": "\n6.3 Mode",
    "text": "6.3 Mode\nThe mode is the most frequently occurring value in a data set. A data set can have one mode (unimodal), multiple modes (multimodal), or no mode at all if all values are unique. The mode is particularly useful for categorical data, such as identifying the most popular product in a sales dataset or the most common salary range in a workforce. In a histogram, the mode corresponds to the peak of the distribution, highlighting where data points concentrate the most.\nExample: Income\nThe most frequently occurring values in the income example is 24.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/central-measures.html#which-measure-should-you-choose",
    "href": "descriptive-stats/central-measures.html#which-measure-should-you-choose",
    "title": "\n6  Measures of Central Tendency\n",
    "section": "\n6.4 Which Measure Should You Choose?",
    "text": "6.4 Which Measure Should You Choose?\nThe appropriate measure of central tendency depends on the data type:\n\n\nData Type\nSuitable Measure(s)\n\n\n\n\nNominal Data (categories)\nMode\n\n\n\nOrdinal Data (ranked categories)\nMode, Median\n\n\n\nInterval Data (e.g., temperature)\nMode, Median, Mean\n\n\n\nRatio Data (e.g., income, weight)\nMode, Median, Mean\n\n\n\nWhen deciding between the mean and median, the mean is preferred for normally distributed data without outliers, while the median is better suited for skewed distributions or data sets with extreme values since it is not influenced by outliers. The mode, while useful for categorical and multimodal data, may not always provide meaningful insights in numerical data sets.\nIn our example, the median income (42 thousand euros) is a better representation of a “typical” income than the mean (44.7 thousand euros) due to high-income values skewing the mean upward.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Measures of Central Tendency</span>"
    ]
  },
  {
    "objectID": "introduction/intro-math.html",
    "href": "introduction/intro-math.html",
    "title": "2  The Unavoidable Math",
    "section": "",
    "text": "2.1 The Sum and The Product\nWe write the sum of \\(n\\) numbers denoted \\(x_1,x_2,\\ldots,x_n\\) as \\[\\sum_{i=1}^n x_i = x_1 +x_2 + \\cdots + x_n \\] This is read as the sum of \\(x_i\\) where \\(i\\) goes from 1 to \\(n\\). The letter \\(i\\) is called the summation index and can be chosen to be any other letter.\nSimilarly, the product of \\(n\\) numbers denoted \\(x_1,x_2,\\ldots,x_n\\) is written as \\[\\prod_{i=1}^n x_i = x_1 \\times x_2 \\times  \\cdots \\times x_n \\].",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Unavoidable Math</span>"
    ]
  },
  {
    "objectID": "introduction/intro-math.html#the-sum-and-the-product",
    "href": "introduction/intro-math.html#the-sum-and-the-product",
    "title": "2  The Unavoidable Math",
    "section": "",
    "text": "2.1.1 Example\nAssume 5 values on \\(x\\) denoted \\(x_1,x_2,x_3,x_4,x_5\\). How can we write the sum of the squared difference of each of these values to their mean value \\(\\overline{x}\\)? \\[(x_1-\\overline{x})^2 + (x_2-\\overline{x})^2 + (x_3-\\overline{x})^2 + (x_4-\\overline{x})^2 + (x_1-\\overline{x})^5 \\]\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[\\sum_{i=1}^n (x_i-\\overline{x})^2\\]",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Unavoidable Math</span>"
    ]
  },
  {
    "objectID": "introduction/intro-math.html#combinatorics",
    "href": "introduction/intro-math.html#combinatorics",
    "title": "2  The Unavoidable Math",
    "section": "2.2 Combinatorics",
    "text": "2.2 Combinatorics\nThe next couple of mathematical concepts covered here are closely linked to the theory of probability which we will cover later in this book. Combinatorics is about counting the number of possibilities to do something. In how many ways?, is a common question here. The rule of product, also knows as the multiplication principle, is a basic counting principle (a.k.a. the fundamental principle of counting). Assume that you have to perform \\(k\\) tasks in turn (one after the other). The first task can be performed in \\(n_1\\) different ways, the second in \\(n_2\\) different ways, etc. In how many different ways can one perform the \\(k\\) tasks in turn? The number of possible ways to perform the \\(k\\) tasks in turn is given by \\[n_1 \\times n_2 \\times  \\cdots \\times n_k\\]\n\n2.2.1 Example\nAssume you are looking at a menu with 3 starters, 4 main courses and 2 desserts. In how many ways can a three-course meal be composed?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[3\\cdot   4 \\cdot 2 = 24\\]",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Unavoidable Math</span>"
    ]
  },
  {
    "objectID": "introduction/intro-math.html#permutations",
    "href": "introduction/intro-math.html#permutations",
    "title": "2  The Unavoidable Math",
    "section": "2.3 Permutations",
    "text": "2.3 Permutations\nPermutations refers to the mathematical calculation of the number of ways a particular set can be arranged. An arrangement of \\(n\\) different objects in a specific order is called a permutation of the objects. The number of permutations that can be formed from \\(n\\) different objects is \\[n! = n\\cdot (n-1)\\cdot (n-2)\\cdot 2\\cdot 1 \\] This is read as \\(n\\)-factorial.\n\n\n\n\n\n\nNote\n\n\n\n\\[ 0!=1 \\]\n\n\n\n2.3.1 Example\nIn how many different ways can we permute the three objects \\(A\\), \\(B\\) and \\(C\\)?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\\[3! = 3 \\cdot 2 \\cdot 1 = 6\\] namely: \\(ABC,\\  ACB, \\ BAC, \\ BCA,\\  CAB, \\ CBA\\)",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>The Unavoidable Math</span>"
    ]
  },
  {
    "objectID": "introduction/intro-surveys.html",
    "href": "introduction/intro-surveys.html",
    "title": "3  Surveys: Key Concepts",
    "section": "",
    "text": "3.1 Census vs. Sample Surveys\nThe population refers to the complete set of elements (individuals, objects, or units) relevant to the study. The definition of a population can vary based on the research objective and can be finite (e.g., employees in a company) or infinite (e.g., potential customers in a market). A sample is a selected subset of the population. (inlcude figure)\nA census (or total survey) involves collecting data from every individual in a given population. This is common in national population counts but is often impractical for other types of research due to cost and time constraints. Instead, most studies rely on sample surveys which aim to generalize findings from the sample to the entire population with reasonable accuracy. The size of the sample (sample size) plays a crucial role in determining the reliability of the conclusions drawn.\nOne way to understand sampling is through the urn metaphor (Figure 3.1): Imagine an urn filled with different-colored balls representing different individuals in a population. Drawing balls at random with/without replacement simulates the process of selecting a sample from a population, emphasizing the role of randomness in reducing bias.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Surveys: Key Concepts</span>"
    ]
  },
  {
    "objectID": "introduction/intro-surveys.html#census-vs.-sample-surveys",
    "href": "introduction/intro-surveys.html#census-vs.-sample-surveys",
    "title": "3  Surveys: Key Concepts",
    "section": "",
    "text": "Figure 3.1: The urn metaphor.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Surveys: Key Concepts</span>"
    ]
  },
  {
    "objectID": "introduction/intro-surveys.html#characteristics-of-a-population",
    "href": "introduction/intro-surveys.html#characteristics-of-a-population",
    "title": "3  Surveys: Key Concepts",
    "section": "3.2 Characteristics of a Population",
    "text": "3.2 Characteristics of a Population\nEach individual in a population has measurable attributes, such as height, weight, income, or opinions. When measuring a particular characteristic, we can calculate various population metrics such as:\n\nMean (average), e.g., the average height of individuals in a population.\nProportion, e.g., the percentage of women in a population.\nTotal values, e.g., the total number of items owned by a group.\nCounts of specific attributes, e.g., the number of people with a particular qualification.\n\nThe main challenge in survey research is determining how accurately a sample represents the entire population. Statistical methods help estimate key characteristics of a population based on sample data.\nThese population characteristics are fixed parameters values which are usually unknown. Common parameters are the mean (\\(\\mu\\)) representing the true average of a characteristic in the population, or the proportion (\\(p\\)) representing the fraction of the population with a certain attribute.\nSince parameters cannot always be measured directly, they are estimated using sample statistics. For example, the sample mean (\\(\\bar{x}\\)) based on sampled observations \\(x_1,x_2,\\ldots, x_n\\) is used to estimate the population mean (\\(\\mu\\)). (include figure)\nTo distinguish between parameters and estimates, Greek letters are typically used for population parameters, while Latin letters are used for sample estimates. We will however avoid using already taken parameters, for example \\(\\pi\\) is not here used for population parameter as this already has a value assigned to it. In such cases we use the Latin letter to indicate the parameter and a ‘hat’ over the letter indicate the sample estimate (that is \\(\\hat{p}\\)).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Surveys: Key Concepts</span>"
    ]
  },
  {
    "objectID": "introduction/intro-surveys.html#types-of-surveys",
    "href": "introduction/intro-surveys.html#types-of-surveys",
    "title": "3  Surveys: Key Concepts",
    "section": "3.3 Types of Surveys",
    "text": "3.3 Types of Surveys\nThe method chosen for collecting data in a survey plays a crucial role in determining the accuracy and reliability of the results. Broadly, survey research can be classified into experimental and non-experimental approaches, each serving different research purposes.\n\n3.3.1 Experimental Surveys\nExperimental surveys are designed to explore causal relationships between variables.Here, researchers have control over certain conditions, manipulating one or more variables while keeping others constant to observe the effects. A key feature of experimental surveys is randomization, where participants are randomly assigned to different groups to eliminate bias. By ensuring that external factors do not influence the results, researchers can draw strong conclusions about cause and effect.\nOne common application of experimental surveys is in medical research, where clinical trials are conducted to test the effectiveness of a new drug. In such cases, patients might be randomly assigned to either a treatment group receiving the drug or a control group receiving a placebo. The outcomes are then compared to determine the drug’s efficacy. Similarly, in marketing, companies may experiment with different advertising strategies by exposing randomly selected groups to different promotional campaigns and then measuring their purchasing behavior.\nWhile experimental surveys provide strong evidence of causal relationships, they do have some limitations. They tend to be resource-intensive, requiring significant time and financial investment. Furthermore, ethical considerations may restrict certain types of experiments, especially in cases where withholding treatment or intervention from a control group could have serious consequences. Another challenge is that controlled settings may not fully capture real-world complexities, making it difficult to generalize findings beyond the experimental conditions.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Surveys: Key Concepts</span>"
    ]
  },
  {
    "objectID": "introduction/intro-surveys.html#non-experimental-surveys-observing-trends-and-patterns",
    "href": "introduction/intro-surveys.html#non-experimental-surveys-observing-trends-and-patterns",
    "title": "3  Surveys: Key Concepts",
    "section": "3.4 Non-Experimental Surveys: Observing Trends and Patterns",
    "text": "3.4 Non-Experimental Surveys: Observing Trends and Patterns\nUnlike experimental surveys, non-experimental surveys focus on observing and describing characteristics, trends, and relationships within a population without direct intervention. These surveys are widely used in fields such as social sciences, market research, and public policy analysis, where the goal is often to collect descriptive data rather than establish causality.\nOne of the most common types of non-experimental surveys is the cross-sectional survey, which captures data from a population at a single point in time. This method is frequently used in opinion polls, customer satisfaction studies, and demographic research. For example, a company might conduct a survey to understand consumer preferences for a new product just before its launch. Because cross-sectional surveys are quick and cost-effective, they are widely used.\nFor studies that require tracking changes over time, researchers may turn to longitudinal surveys, which collect data from the same subjects at multiple intervals. Longitudinal surveys are especially useful for understanding long-term trends, such as how consumer behavior evolves over the years or how health outcomes change in response to lifestyle choices. In a panel study, the same individuals are followed over time, whereas in a cohort study, a specific group—such as people born in a particular year—is tracked to observe changes as they age. These methods are valuable in policy research, where understanding the long-term effects of interventions, such as educational reforms or public health initiatives, is critical.\nSome non-experimental surveys rely on observational data, where researchers study behaviors and interactions without directly questioning participants. Observational studies often aim to identify associations and generate hypotheses for further research. This method is commonly used in consumer behavior research. For example, a supermarket might analyze shopping patterns by tracking how customers navigate store aisles without them being aware of the observation. Another example is web tracking which is a modern form of observational research where companies monitor users’ online activities to analyze behavior, predict preferences, and personalize experiences. Unlike traditional observational studies, which are typically conducted with ethical oversight and clear participant consent, the presented examples often operates in the background without users’ full awareness or control. Thus, while observational studies provide authentic behavioral insights, they raise ethical concerns about privacy and cannot establish direct cause-and-effect relationships.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Surveys: Key Concepts</span>"
    ]
  },
  {
    "objectID": "introduction/variable-class.html",
    "href": "introduction/variable-class.html",
    "title": "4  Variable Classification",
    "section": "",
    "text": "4.1 Types of Variables\nTo facilitate analysis, we focus on measurable variables, which are broadly categorized into quantitative and qualitative types. There are further sub-classes for each of these main classes, as shown in Figure 4.1 and exemplified further in the following.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variable Classification</span>"
    ]
  },
  {
    "objectID": "introduction/variable-class.html#types-of-variables",
    "href": "introduction/variable-class.html#types-of-variables",
    "title": "4  Variable Classification",
    "section": "",
    "text": "graph TD;\n    A(Variable) --&gt; B(Qualitative);\n    A(Variable) --&gt; C(Quantitative);\n    C --&gt; D(Discrete);\n    C --&gt; E(Continuous);\n\n\n\n\n\n\n\n\nFigure 4.1: Variable classification flowchart.\n\n\n\n\n4.1.1 Quantitative (Numerical)\nThese variables are represented by numbers and can be measured. Depending on the type of numbers a variable takes, it can be classified as discrete or continuous.\nDiscrete variables take specific, distinct values and cannot be subdivided (natural, integer, or rational numbers). Think of these as variables holding countable values. Examples include number of children in a family, number of goals ina football match, and number of sales transactions per day.\nContinuous variables can take any value within a given range of values within an interval and can be infinitely divided (real numbers). Examples include hegiht, weight, stock price and distance.\n\n\n4.1.2 Qualitative (Categorical) Variables\nThese variables represent data that can be divided into distinct groups or categories. These values do not have a natural numerical order (except for ordinal variables) and must be coded into numerical values for statistical analysis. Examples include political affiliation, blood type, movie genre and social media platform.\nA special case of qualitative variables that take only two possible values, so called dichotomous or binary variable. Examples include smoking status (Smoker/Non-smoker) and COVID-19 test result (positive/negative).",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variable Classification</span>"
    ]
  },
  {
    "objectID": "introduction/variable-class.html#levels-of-measurement",
    "href": "introduction/variable-class.html#levels-of-measurement",
    "title": "4  Variable Classification",
    "section": "4.2 Levels of Measurement",
    "text": "4.2 Levels of Measurement\nThe characteristics of collected data allow us to classify them into four different measurement levels. These levels determine the statistical operations that can be performed. Table 4.1 summarizes the different measurement levels described in the following.\nNominal scale categorizes data without any inherent order. For example, there is no inherent ordering in the variable eye color (blue, brown, green), gender or nationality. You can only distinguish between the values nominal variables take.\nOrdinal scale is when data can be ranked in a meaningful order, but differences between values are not necessarily equal or meaningful, for example when looking at the variable fruit preference ranking or customer satisfaction levels (low, medium, high). In other words, interval lengths between one variable value and another are not of the same length.\nInterval scale is similar to the ordinal scale but with equal intervals between values. However, it lacks a true zero point. Examples include Temperature in Celsius and calendar years (the year 2000 is 100 years after 1900, but the year 0 does not represent the “beginning of time”).\nFinally, ratio scale has all of the above properties but also a natural zero point, allowing meaningful calculations of differences and ratios. Examples here include salary, distance traveled, height, and weight.\n\n\n\nTable 4.1: Table Summary of Measurement Levels\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistinguish\nRank\nEqual Step Length\nAbsolute Zero Point\nExample\n\n\n\n\nNominal\nYes\nNo\nNo\nNo\ngender, city, religion\n\n\nOrdinal\nYes\nYes\nNo\nNo\ngrades, preference, customer satisfaction ratings\n\n\nInterval\nYes\nYes\nYes\nNo\ntemperature in Celsius. credit scores, calender years\n\n\nRatio\nYes\nYes\nYes\nYes\nlength, weight, time, salary",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variable Classification</span>"
    ]
  },
  {
    "objectID": "introduction/variable-class.html#types-of-numbers",
    "href": "introduction/variable-class.html#types-of-numbers",
    "title": "4  Variable Classification",
    "section": "4.3 Types of Numbers",
    "text": "4.3 Types of Numbers\nUnderstanding the nature of numbers is crucial when classifying variables because it directly influences statistical analysis, measurement accuracy, and the interpretation of data. The classification of numbers into natural, whole, integer, rational, irrational, and real numbers helps in determining which mathematical operations and statistical techniques are valid for a given data set:\n\nNatural Numbers: \\(0, 1, 2, 3, \\ldots\\)\nIntegers: \\(\\dots , -3, -2, -1, 0, 1, 2, 3, \\ldots\\)\nRational Numbers: Numbers that can be expressed as a fraction \\(\\frac{a}{b}\\) , where \\(a\\) and \\(b\\) are integers. Examples include:\n\n\\(-14 = \\frac{-14}{1}\\)\n\\(\\frac{3}{4} = 0.75\\)\n\\(\\frac{2}{7} = 0.285714285714 \\dots\\)\n\nReal Numbers: Non-repeating decimal numbers, such as:\n\n\\(\\pi = 3.14159265358979 \\dots\\)\n\n\nFor example you cannot calculate an average zip code (nominal) or say that “a temperature of 20°C is twice as hot as 10°C” (interval), but you can say “a person earning €50,000 earns twice as much as someone earning €25,000” (ratio). An another example, you cannot consider the mean of a categorical variable like “favorite color,” but you can analyze the frequency distribution.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Variable Classification</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/describe-data.html",
    "href": "descriptive-stats/describe-data.html",
    "title": "\n5  Describing a Dataset\n",
    "section": "",
    "text": "5.1 Describing Qualitative Variables\nQualitative (categorical) variables represent data grouped into distinct categories, such as gender, marital status, or election participation. These variables are best summarized using frequency tables and graphs such as bar charts and pie charts.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Describing a Dataset</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/describe-data.html#describing-qualitative-variables",
    "href": "descriptive-stats/describe-data.html#describing-qualitative-variables",
    "title": "\n5  Describing a Dataset\n",
    "section": "",
    "text": "5.1.1 Frequency Tables for Qualitative Data\nA frequency table lists the categories of a variable along with their corresponding counts (absolute frequency) and percentages (relative frequency). Assume we surveyed 300 people about their favorite hot beverage and found that 60% prefer coffee and 40% prefer tea. This preference distribution, also presented in the table below, will be used as a running example in the following.\n\n\nPreference\nCount (\\(f_i\\))\nPercentage (\\(f_i\\) in %)\n\n\n\nTea\n120\n40%\n\n\nCoffee\n180\n60%\n\n\nTotal\n300\n100%\n\n\n\nThe relative frequency is calculated as:\n\\[\\frac{120}{300} \\times 100 = 40\\%\\]\nand shown in the third column.\n\n5.1.2 Pie Chart\nPie charts are one of the most commonly used tools for representing categorical data in a simple, visual format. They break down a whole into proportional slices, making it easy to see relative differences between categories at a glance. Whether you’re comparing sales across different product categories, analyzing survey responses, or breaking down a budget, a well-made pie chart provides an intuitive way to present proportions.\nEach slice represents a category’s percentage of the total, with the entire pie equaling 100%. The size of each slice is determined by the proportion of the category it represents. For example, if 60% of survey respondents prefer coffee over tea, that category would take up 60% of the pie chart, or 60% of 360°, that is \\(0.60 × 360° = 216°\\) of the full circle. Pie charts are particularly effective when comparing a few distinct categories but lose clarity when too many slices are included.\n\n\n\n\n\n\n\n\nWhile a standard pie chart is a great way to visualize data, 3D pie charts are the dark side of data visualization. They may look fancy, but they distort proportions, making it difficult to accurately compare slice sizes. Due to the perspective effect, some slices appear larger or smaller than they actually are, leading to misleading interpretations. In short: if you want your data to be clear and not just flashy, stick to 2D pies - your audience will thank you.\n\n5.1.3 Bar Chart\nBar charts are a great way to visualize qualitative data, making it easy to compare different categories. Each category is represented by a bar, with the height corresponding to its frequency or percentage. For example, a bar chart would clearly display the difference between coffee and tea lovers, making it easy to interpret at a glance. Unlike pie charts, bar charts work well even when multiple categories are involved, ensuring your audience can quickly grasp the data - without any risk of 3D chart-induced confusion!\n\n\n\n\n\n\n\n\n\n5.1.4 Contingency Tables\nWhen we have observations on two qualitative variables, we can create two separate frequency tables. However, if we want to study the relationship between the two variables, we use a contingency table.\nA contingency table organizes paired observations, showing the frequency distribution across the two categorical variables.\nFor example, consider the following data where individuals are classified into two groups based on their marital status (Married (M) or Not Married (NM)) and their voting behavior (Voted (1) or Did Not Vote (0)). This results in four possible outcome combinations:\n\n(M, 0) - Married, Did Not Vote\n(M, 1) - Married, Voted\n(NM, 0) - Not Married, Did Not Vote\n(NM, 1) - Not Married, Voted\n\nAssume the first four observations are: (G, 0), (EG, 1), (G, 1), (G, 1) Which we create the following table over:\n\n\n\n0 (Did Not Vote)\n1 (Voted)\n\n\n\nM (Married)\n✔️\n✔️✔️\n\n\nNM (Not Married)\n\n✔️\n\n\n\nOnce all observations have been recorded, we can create the following contingency table:\n\n\n\nDid Not Vote\nVoted\nTotal\n\n\n\nMarried\n54\n1496\n1550\n\n\nNot Married\n85\n628\n713\n\n\nTotal\n139\n2124\n2263\n\n\n\nThis table displays the distribution of voting behavior by marital status, where we can analyze differences between the groups.\nMarginal Distributions\nA marginal distribution summarizes the totals for each row and column in a contingency table. This helps us understand the overall distribution of each variable separately.\nFor example, the absolute and relative marginal distributions for marital status is shown below:\n\n\nMarital Status\nCount\nPercentage (%)\n\n\n\nMarried\n1550\n68.5%\n\n\nNot Married\n713\n31.5%\n\n\nTotal\n2263\n100%\n\n\n\nSimilarly, the absolute and relative marginal distributions for voting behvaior is shown below:\n\n\nVoting Behavior\nCount\nPercentage (%)\n\n\n\nDid Not Vote\n139\n6.1%\n\n\nVoted\n2124\n93.9%\n\n\nTotal\n2263\n100%\n\n\n\nThese tables summarize how many people are in each category without considering the second variable.\nConditional Distributions\nTo compare voting behavior between married and non-married individuals, we calculate row percentages.\n\n\n\nDid Not Vote (%)\nVoted (%)\nTotal (%)\n\n\n\nMarried\n3.5%\n96.5%\n100%\n\n\nNot Married\n11.9%\n88.1%\n100%\n\n\n\nThis table shows the conditional distribution of voting behavior, given marital status.\n\nAmong married individuals, 1496 × 100 = 0.965= 96.5% voted while 54 × 100 = 0.035 =3.5% did not.\nAmong non-married individuals, 88.1% voted, while 11.9% did not.\n\nBy comparing these row percentages, we can see that married individuals were more likely to vote compared to non-married individuals. We have calculated the percentages horizontally but compare the percentage values in the vertical columns. We can also compute column percentages instead of row percentages if needed.\nTo determine whether a relationship exists between voting behavior and marital status, we compare conditional distributions. Since the voting percentages differ between married and non-married groups, we conclude that marital status influences voting behavior. If the two variables were independent, the percentages in the columns would be nearly the same. The fact that they differ suggests an association between the two variables.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Describing a Dataset</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/describe-data.html#describing-quantitative-variables",
    "href": "descriptive-stats/describe-data.html#describing-quantitative-variables",
    "title": "\n5  Describing a Dataset\n",
    "section": "\n5.2 Describing Quantitative Variables",
    "text": "5.2 Describing Quantitative Variables\nUnlike qualitative data, which can be subjective and harder to categorize, quantitative data enables direct comparisons, making it easier to identify patterns, test hypotheses, and make data-driven decisions.\nQuantitative variables are numeric and can be either discrete (specific values, such as test scores) or continuous (measured on a scale, such as weight). These variables are best summarized using frequency tables, histograms, and cumulative distributions.\n\n5.2.1 Frequency and Cumulative Frequency Tables\nExample: Mathematics Grade\nWe begin by examining a discrete variable with a small number of observations. Assume the mathematics grades (ranging from 1-5) of 25 students are:\n5 4 1 4 4 3 2 3 3 3 4 2 3 1 3 3 5 4 2 2 2 4 3 5 3\nWhen data is presented in this way, it is referred to as ungrouped data. Let:\n\n\n\\(x_i\\) = observed values, where \\(i = 1, \\ldots , n\\)\n\n\n\\(f_i\\) = frequency of the \\(i\\)-th variable value, where \\(i = 1, 2, \\ldots, k\\).\n\nFor our example here, \\(n = 25\\) (total observations) and \\(k = 5\\) (five distinct values of the variable “Mathematics Grades”).\nLet’s summarize this data by counting how many we have in each grade category:\n\n\nGrade (\\(x_i\\))\nCount\nFrequency (\\(f_i\\))\n\n\n\n1\n✔️✔️\n2\n\n\n2\n✔️✔️✔️✔️✔️\n5\n\n\n3\n✔️✔️✔️✔️✔️✔️✔️✔️✔\n9\n\n\n4\n✔️✔️✔️✔️✔️✔️\n6\n\n\n5\n✔️✔️✔️\n3\n\n\nTotal\n\n25\n\n\n\nThe sum of all frequencies equals the total number of observations: [ _{i=1}^{k} f_i = n ]\nWe have created a frequency table by grouping the data into categories which can be visualized using a bar chart:\n\n\n\n\n\n\n\n\nThe cumulative frequency tells us how many observations are less than or equal to a given value.\n\n\n\n\n\n\n\nScore (\\(x_i\\))\nAbsolute Frequency (\\(f_i\\))\nCumulative Frequency (\\(F_i\\))\n\n\n\n1\n2\n2\n\n\n2\n5\n7\n\n\n3\n9\n16\n\n\n4\n6\n22\n\n\n5\n3\n25\n\n\n\nCumulative frequencies are often displayed using a cumulative step graph:",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Describing a Dataset</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/describe-data.html#histograms",
    "href": "descriptive-stats/describe-data.html#histograms",
    "title": "\n5  Describing a Dataset\n",
    "section": "\n5.3 Histograms",
    "text": "5.3 Histograms\nWhen dealing with a continuous variable or a discrete variable with many values, it is common to create class intervals and then display frequencies in a frequency table or graph.\nExample: Candy Bar Weights\n\n\n\n\nWe have observed 40 candy bars of a specific brand and recorded their weighs which are given in the following in ascending order:\n20.5 20.7 20.8 21.0 21.0 21.4 21.5 22.0 22.1 22.5\n22.6 22.6 22.7 22.7 22.9 22.9 23.1 23.3 23.4 23.5\n23.6 23.6 23.6 23.9 24.1 24.3 24.5 24.5 24.8 24.8\n24.9 24.9 25.1 25.1 25.2 25.6 25.8 25.9 26.1 26.7\nSince weight is a continuous variable, we must group the observations into classes. We choose five class intervals, each with a width of 1.3 grams, starting from 20.4 grams:\n\n\nClass 1: 20.4 - 21.6\n\nClass 2: 21.7 - 22.9\n\nClass 3: 23.0 - 24.2\n\nClass 4: 24.3 - 25.5\n\nClass 5: 25.6 - 26.9\n\nWe can then create a frequency table as before:\n\n\nWeight Range (grams)\nFrequency (\\(f_i\\))\n\n\n\n20.4 - 21.6\n7\n\n\n21.7 - 22.9\n9\n\n\n23.0 - 24.2\n9\n\n\n24.3 - 25.5\n10\n\n\n25.6 - 26.9\n5\n\n\nTotal\n40\n\n\n\nWe then can visualize the frequency distribution using a histogram:\n\n\n\n\n\n\n\n\nGenerally, A histogram represents continuous data by grouping values into intervals, with bar heights corresponding to frequencies.\nTo determine how many observations fall below a given value, we calculate the cumulative frequencies as before and visualize using a step chart.\n\n\n\n\n\n\n\n\n\nWeight Range (grams)\nAbsolute Frequency (fi)\nCumulative Frequency (Fi)\nRelative Frequency (%)\nCumulative Relative Frequency (%)\n\n\n\n20.4 - 21.6\n7\n7\n17.5\n17.5\n\n\n21.7 - 22.9\n9\n16\n22.5\n40.0\n\n\n23.0 - 24.2\n9\n25\n22.5\n62.5\n\n\n24.3 - 25.5\n10\n35\n25.0\n87.5\n\n\n25.6 - 26.9\n5\n40\n12.5\n100\n\n\nTotal\n40\n40\n100%\n100%",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Describing a Dataset</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/describe-data.html#stem-and-leaf-plot",
    "href": "descriptive-stats/describe-data.html#stem-and-leaf-plot",
    "title": "\n5  Describing a Dataset\n",
    "section": "\n5.4 Stem-and-Leaf Plot",
    "text": "5.4 Stem-and-Leaf Plot\nA Stem-and-leaf plot is a compact way to display numerical data while preserving individual values. It organizes data into stems (representing the leading digits) and leaves (the following digits), providing a good display of the distribution.\nFor example, in the dataset of candy bar weights, a stem-and-leaf plot can show whether weights cluster around a certain value and help identify any inconsistencies. This is shown in the follwing.\nWe split each value from our candy bar weight dataset into - Stem (e.g., 20, 21, 22, etc.) - Leaf (the decimal part, such as .1, .2, .3, etc.)\nThe Stem-and-Leaf Table is then given as:\n\n\nStem\nLeaf\n\n\n\n20\n5 7 8\n\n\n21\n0 0 4 5\n\n\n22\n0 1 5 6 6 7 7 9 9\n\n\n23\n1 3 4 5 6 6 6 9\n\n\n24\n1 3 5 5 8 8 9 9\n\n\n25\n1 1 2 6 8 9\n\n\n26\n1 7\n\n\n\nIf you tilt your head to the right or rotate the table 90° you get a fairly good view on the distribution of the data. The distribution appears fairly symmetric, with a slight skew toward the higher weights. Overall, the data is well distributed across the entire range, but there is a higher density of observations between 22.0 g and 24.9 g, indicating that the most frequent range appears to be 22 to 24 grams, with many values concentrated in these stems. The least frequent weights occur at the lower (20-21 g) and higher (25-26 g) ends. In a quality control one might check this distribution and note whether a large amount of candy bars end up in the tails of the distribution, thus indicating inconsistent production of candy bars.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Describing a Dataset</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/dispersion-measures.html",
    "href": "descriptive-stats/dispersion-measures.html",
    "title": "\n7  Measures of Dispersion\n",
    "section": "",
    "text": "Example: Test Scores\nConsider two sets of scores from two different groups of students. Each data set contains eight observations, representing the scores students received on a test:\nData A\nData B\nBoth data sets have the same mean: \\[\\bar{x}_A = \\frac{4 + 4 + 5 + 5 + 5 + 6 + 6 + 7}{8} = 5.25\\] \\[\\bar{x}_B = \\frac{0+1+4+5+6+7+8+11}{8} = 5.25\\]\nHowever, data set B has a much wider spread of values, ranging from 0 to 11, while data set A is more compact, with values between 4 and 7. The greater spread in data set B suggests higher variability in scores, meaning individual performances were less consistent compared to data set A. In contrast, data set A shows more uniform performance, suggesting students’ scores were relatively close to each other. The two data set distributions are visualized below:\nSeveral statistical measures help quantify dispersion in a dataset, some of which are covered in the following.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/dispersion-measures.html#quartiles-and-percentiles",
    "href": "descriptive-stats/dispersion-measures.html#quartiles-and-percentiles",
    "title": "\n7  Measures of Dispersion\n",
    "section": "\n7.1 Quartiles and Percentiles",
    "text": "7.1 Quartiles and Percentiles\nQuartiles and percentiles divide data into sections, helping us understand the distribution more effectively. The most commonly used quartiles are the first quartile (Q1), median (Q2), and third quartile (Q3).\nFirst Quartile (Q1) - 25th Percentile\nThe first quartile (Q1) marks the value below which 25% of the observations fall. It helps us understand the lower range of the dataset and is computed as: \\[\nQ1 = \\text{value at position } 0.25(n+1)\n\\] where \\(n\\) is the total number of observations.\nSecond Quartile (Q2) – 50th Percentile (Median)\nThe second quartile (Q2) is simply the median, dividing the dataset into two equal halves. This is calculated as: \\[\nQ2 = \\text{value at position } 0.50(n+1)\n\\] Since 50% of values are below this point, the median represents the central value in the distribution.\nThird Quartile (Q3) - 75th Percentile\nThe third quartile (Q3) is the value below which 75% of the observations fall. This is particularly useful for understanding the upper range of the dataset and is calculated as: \\[\nQ3 = \\text{value at position } 0.75(n+1)\n\\]\nQuartiles provide valuable information about how data is spread across different sections. They allow us to:\n\nIdentify skewness: If Q1 and Q3 are unevenly spaced around Q2 (the median), the data may be skewed.\nDetect outliers: Any value that is significantly lower than Q1 or higher than Q3 can be considered an outlier.\nCalculate the Interquartile Range (\\(IQR\\)), which is the difference between Q3 and Q1, providing a robust measure of spread that is less sensitive to extreme values (not to be confused with range which is the difference between minimum and maximum observation values \\(x_{max}-x_{min}\\)).\nCalculate the Quartile Deviation which is another measure that is robust against extreme values and defined as half the difference between the third quartile (Q3) and the first quartile (Q1): \\[\\frac{Q3-Q1}{2} .\\]",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/dispersion-measures.html#five-number-summary-and-boxplot",
    "href": "descriptive-stats/dispersion-measures.html#five-number-summary-and-boxplot",
    "title": "\n7  Measures of Dispersion\n",
    "section": "\n7.2 Five-Number Summary and Boxplot",
    "text": "7.2 Five-Number Summary and Boxplot\nA Five-Number Summary is a set of five descriptive statistics that provide insights into the distribution of a data set. These include:\n\n\nMinimum – The smallest observed value.\n\nFirst Quartile (Q1) – The 25th percentile, below which 25% of the data falls.\n\nSecond Quartile (Median, Q2) – The 50th percentile, the middle value of the data set.\n\nThird Quartile (Q3) – The 75th percentile, below which 75% of the data falls.\n\nMaximum – The largest observed value.\n\nThe Five-Number Summary helps in constructing a boxplot, which visually represents the spread and skewness of the data, as well as potential outliers. An example is shown in Figure 7.1.\nThe boxplot visually represents the distribution and spread of the data using the five-number summary. The minimum and maximum values mark the range of the data, while the first quartile (Q1), median (Q2), and third quartile (Q3) divide the data into four equal parts. The interquartile range (IQR), which spans from Q1 to Q3, highlights the middle 50% of the data, giving insights into variability.\nThe median (Q2) represents the central value, while outliers (if any) are shown as red points beyond the whiskers of the box. This boxplot effectively summarizes the data set, making it easy to identify skewness, dispersion, and potential outliers at a glance.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.1: Boxplot with labels for each component of the Five-Number Summary\n\n\nExample: Test Scores\nWe compute the Five-Number Summary for the two datasets, A and B, representing test scores.\nData A\n4 4 5 5 5 6 6 7\nData B\n0 1 4 5 6 7 8 11\n1 and 5: The range is the difference between the maximum and minimum values (thus allowig us to see the minimum and maximum as well):\n\nA: (7 - 4 = 3)\nB: (11 - 0 = 11)\n\n2: The first quartile is found at position:\\[\nQ1 = 0.25(n+1) = 0.25(9) = 2.25\n\\]\n\nA: \\(Q1 = 4 + 0.25(5-4) = 4.25\\)\n\nB: \\(Q1 = 1 + 0.25(4-1) = 1.75\\)\n\n\n3: The median is found at position:\\[\nQ2 = 0.50(n+1) = 0.50(9) = 4.5\n\\]\n\nA: \\(Q2 = 5\\)\n\nB: \\(Q2 = 5.5\\)\n\n\n4: The third quartile is found at position:\\[\nQ3 = 0.75(n+1) = 0.75(9) = 6.75\n\\]\n\nA: \\(Q3 = 6 + 0.75(6-6) = 6\\)\n\nB: \\(Q3 = 7 + 0.75(8-7) = 7.75\\)\n\n\nFinal Five-Number Summaries\n\n\nDataset\nMinimum\nQ1\nMedian (Q2)\nQ3\nMaximum\n\n\n\nA\n4\n4.25\n5\n6\n7\n\n\nB\n0\n1.75\n5.5\n7.75\n11\n\n\n\nTo better understand the distribution of the two datasets, we use a boxplot to visualize the Five-Number Summary.\n\n\n\n\n\n\n\n\nThe boxplot visually highlights key aspects of dispersion, skewness, and potential outliers. In our example:\n\nDataset A has a smaller range (3) and is more compact.\nDataset B has a wider range (11), indicating greater variability in scores.\n\nBy using these descriptive statistics, we can better interpret datasets and make informed comparisons in various fields, including education, business, and research.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/dispersion-measures.html#variance-and-standard-deviation",
    "href": "descriptive-stats/dispersion-measures.html#variance-and-standard-deviation",
    "title": "\n7  Measures of Dispersion\n",
    "section": "\n7.3 Variance and Standard Deviation",
    "text": "7.3 Variance and Standard Deviation\nWhen analyzing data, calculating the mean provides insight into the average value of a dataset. However, to understand how spread out the data is, we need to measure its variability. This is where variance and standard deviation become essential.\nVariance quantifies the average squared deviation of each data point from the mean. It provides a measure of how much the data points differ from the central value. For an entire population, the variance (\\(\\sigma^2\\)) is calculated as: \\[\nσ^2 = \\frac{\\sum_{i=1}^{N} (x_i - \\mu)^2}{N}\n\\] where:\n\n\n\\(N\\) = total number of data points in the population,\n\n\\(x_i\\) = individual data points,\n\n\\(\\mu\\) = population mean.\n\nWhen working with a sample instead of an entire population, we use the sample variance (\\(s^2\\)): \\[\ns^2 = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}\n\\] where:\n\n\n\\(n\\) = sample size,\n\n\\(x_i\\) = individual data points,\n\n\\(\\bar{x}\\)= sample mean.\n\nThe denominator \\((n-1)\\) instead of \\(n\\) accounts for the loss of one degree of freedom, making it an unbiased estimator of population variance (we’ll return to this later).\nAn alternative formula for calculating sample variance can be found by noting that the sum of all deviations from the mean is zero: \\(\\sum_{i=1}^{n} x_i = n\\bar{x}\\) so that we get \\[\n\\sum_{i=1}^{n} x_i \\bar{x} = \\bar{x} \\sum_{i=1}^{n} x_i = n \\bar{x}^2.\n\\] Substituting this back into original equation yields: \\[\ns^2 = \\frac{\\sum_{i=1}^{n} x_i^2 - 2n\\bar{x}^2 + n\\bar{x}^2}{n-1}\n= \\frac{\\sum_{i=1}^{n} x_i^2 - n\\bar{x}^2}{n-1}\n\\] Rewriting using summation notation: \\[\ns^2 = \\frac{n \\sum_{i=1}^{n} x_i^2 - (\\sum_{i=1}^{n} x_i)^2}{n(n-1)}\n\\] This formulation simplifies calculations by hand when working with moderately large data sets.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/dispersion-measures.html#standard-deviation",
    "href": "descriptive-stats/dispersion-measures.html#standard-deviation",
    "title": "\n7  Measures of Dispersion\n",
    "section": "\n7.4 Standard Deviation",
    "text": "7.4 Standard Deviation\nThe standard deviation is the square root of variance, bringing it back to the same units as the data, for example if measuring weight in kg, standard deviation is in kg as well. Thus, it is better to use when you need an intuitive, practical measure of data spread in real-world scenarios.\nFor a population, the standard deviation (\\(\\sigma\\)) is: \\[\n\\sigma = \\sqrt{\\frac{\\sum_{i=1}^{N} (x_i - \\mu)^2}{N}}\n\\] For a sample, the standard deviation (\\(s\\)) is: \\[\ns = \\sqrt{\\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})^2}{n-1}}\n\\] which can also be rewritten as: \\[\ns = \\sqrt{\\frac{n \\sum_{i=1}^{n} x_i^2 - (\\sum_{i=1}^{n} x_i)^2}{n(n-1)}}\n\\] following the alternative variance formula shown above.\nTo better understand the concept of standard deviation, we visualize the distribution of a data set in Figure 7.2 where the mean (blue) and standard deviation bands (red) are overlayed.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.2: Histogram of data set of 100 0bservations with standard deviation bands included.\n\n\nExample: Test Scores\nReturning to our example on test scores, we previously calculated the sample mean as \\(\\bar{x} = 5.25\\). Now, we compute the variance (\\(s^2\\)) for each data set: \\[\ns^2_A = \\frac{(4−5.25)^2 + (4−5.25)^2 + \\dots + (7−5.25)^2}{8-1}  \\approx 1.074\n\\] \\[\ns^2_B = \\frac{(0−5.25)^2 + (1−5.25)^2 + \\dots + (11−5.25)^2}{8-1} \\approx 13.071\n\\]\nThe standard deviation (\\(s\\)) is then simply the square root of the variance: \\[\ns_A = \\sqrt{1.071} \\approx 1.035\n\\] and \\[\ns_B = \\sqrt{13.071} \\approx 3.615\n\\] We see that for data set A, the standard deviation is 1.035, indicating that most scores are relatively close to the mean (5.25), while for data set B, the standard deviation is 3.615, suggesting a wider spread of scores and greater variability. This comparison shows that data set B has a significantly higher variability than data set A, meaning the scores are more dispersed from the average.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/dispersion-measures.html#the-empirical-rule",
    "href": "descriptive-stats/dispersion-measures.html#the-empirical-rule",
    "title": "\n7  Measures of Dispersion\n",
    "section": "\n7.5 The Empirical Rule",
    "text": "7.5 The Empirical Rule\nThe Empirical Rule, also known as the 68-95-99.7 Rule, describes how data is distributed in a normal (bell-shaped) distribution. It states that for a large population following a normal distribution:\n\n\nApproximately 68% of all observations lie within one standard deviation from the mean (\\(\\mu ± 1\\sigma\\)).\n\nApproximately 95% of all observations lie within two standard deviations from the mean (\\(\\mu ± 2\\sigma\\)).\n\nNearly all observations (99.7%) lie within three standard deviations from the mean (\\(\\mu ± 3\\sigma\\)).\n\nThis rule helps us understand the probability of an observation falling within a given range and is widely used in quality control, finance, and science to assess variability and expected outcomes. It is particularly useful when analyzing data distributions. If data follows a normal distribution most values cluster around the mean, and extreme values are rare. This also means that outliers can be identified if they fall beyond 3 standard deviations from the mean.\nThe empricial rule is visualized in Figure 7.3 showing the percentages of data falling within each standard deviation range.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.3: The Empirical Rule: Normal Distribution.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "descriptive-stats/dispersion-measures.html#covariance-and-correlation",
    "href": "descriptive-stats/dispersion-measures.html#covariance-and-correlation",
    "title": "\n7  Measures of Dispersion\n",
    "section": "\n7.6 Covariance and Correlation",
    "text": "7.6 Covariance and Correlation\nWhen analyzing data, it is often important to understand the relationship between two variables. Measures such as covariance and correlation help quantify the degree to which two variables change together, allowing us to assess their association.\nCovariance measures the direction of the linear relationship between two variables, \\(X\\) and \\(Y\\). It tells us whether an increase in one variable is associated with an increase or decrease in the other. For an entire population, the covariance is calculated as: \\[\nCov(X, Y) = \\sigma_{xy} = \\frac{\\sum_{i=1}^{N} (x_i - \\mu_x)(y_i - \\mu_y)}{N}\n\\] where:\n\n\n\\(N\\) = total number of observations,\n\n\\(x_i, y_i\\) = individual data points,\n\n\\(\\mu_x \\mu_y\\) = means of \\(X\\) and \\(Y\\).\n\nFor a sample, we estimate covariance using: \\[\nCov(X, Y) = s_{xy} = \\frac{\\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})}{n - 1}\n\\] where:\n\n\n\\(n\\) = sample size,\n\n\\(\\bar{x} ,\\bar{y}\\) = sample means of \\(X\\) and \\(Y\\).\n\nHow do we interpret the covariance?\n\nIf we have positive covariance it means that when \\(X\\) increases, then \\(Y\\) also tends to increase (e.g., study time and exam scores).\nIf we have negative covariance it means that when \\(X\\) increases, then \\(Y\\) tends to decrease (e.g., speed and time taken to reach a destination).\nIf we have near zero covariance, then this indicates no significant linear relationship between \\(X\\) and \\(Y\\) (note however that it does not detect patterns where variables are related in a non-linear way e.g., quadratic or exponential relationships).\n\nOne limitation of covariance is that it depends on the units of measurement (same as for variance), making it difficult to interpret. This is where correlation comes in as it standardizes covariance by adjusting for the scales of the variables, providing a dimensionless measure that is easier to interpret. The population correlation (\\(\\rho\\)) is given by \\[\n\\rho = \\frac{Cov(X, Y)}{\\sigma_x \\sigma_y}\n\\] where \\(\\sigma_x,\\sigma_y\\) are the standard deviations of \\(X\\) and \\(Y\\). The sample correlation (\\(r\\)) \\[\nr = \\frac{Cov(X, Y)}{s_x s_y}\n\\] where \\(s_x, s_y\\) are the sample standard deviations. Correlation values fall within the range -1 to 1, with the following interpretations (we use sample correlation as example):\n\n\n\\(r = 1\\): perfect positive correlation; \\(X\\) and \\(Y\\) move together exactly in a straight line.\n\n\\(0.8 \\leq r &lt; 1\\): strong positive correlation; \\(X\\) and \\(Y\\) tend to increase together.\n\n\\(0.5 \\leq r &lt; 0.8\\): moderate positive correlation; \\(X\\) and \\(Y\\) show a noticeable increasing relationship.\n\n\\(0 &lt; r &lt; 0.5\\): weak positive correlation; \\(X\\) and \\(Y\\) tend to increase together, but with variability.\n\n\\(r = 0\\): no linear relationship; \\(X\\) and \\(Y\\) are not linearly related (but might be non-linearly associated).\n\n\\(-0.5 &lt; r &lt; 0\\): weak negative correlation; as \\(X\\) increases, \\(Y\\) tends to decrease slightly.\n\n\\(-0.8 &lt; r \\leq -0.5\\): moderate negative correlation; \\(X\\) and \\(Y\\) shown an inverse relationship.\n\n\\(-1 ≤ r ≤ -0.8\\): strong negative correlation; \\(X\\) and \\(Y\\) move in opposite directions strongly.\n\n\\(r = -1\\): perfect negative correlation; \\(X\\) and \\(Y\\) move in exactly opposite directions in a straight line.\n\nA few examples are shown in Figure 7.4.\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.4: Simulated data showing different correlations.",
    "crumbs": [
      "Descriptive Statistics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Measures of Dispersion</span>"
    ]
  },
  {
    "objectID": "introduction/what-is-stats.html",
    "href": "introduction/what-is-stats.html",
    "title": "1  What is Statistics?",
    "section": "",
    "text": "1.1 Inferential Statistics\nThis is where things get spicy! Instead of just describing what we see, we gain insight or make predictions about the big picture based on a small sample. It’s like tasting one donut from the box and guessing if the whole batch is good 🍩.\nConsider you have a hypothesis you wish to test (alternative hypothesis). Using data collected, you express the rules of the game through probabilities, distributions, and statistical models. This helps create a “toy model” of the world based on your hypothesis. The null hypothesis is then the “default world,” and your working hypothesis is literally everything else. You pretend you know how things work, and then check if reality agrees with you (hypothesis testing). Then you ask the big question: Does our data make the null hypothesis look completely ridiculous? If yes, we might just reject it and revise the null world accordingly.\nStatistics isn’t about finding the ultimate truth, but about making the best possible decisions with the information available. We estimate, test, and adjust—because in the end, being less wrong is the best we can do. In the words of the famous statistician George Box: “All models are wrong, but some are useful”.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Statistics?</span>"
    ]
  },
  {
    "objectID": "introduction/what-is-stats.html#inferential-statistics",
    "href": "introduction/what-is-stats.html#inferential-statistics",
    "title": "1  What is Statistics?",
    "section": "1.2 Inferential Statistics",
    "text": "1.2 Inferential Statistics\nThis is where things get spicy! Instead of just describing what we see, we gain insight or make predictions about the big picture based on a small sample. It’s like tasting one donut from the box and guessing if the whole batch is good 🍩.\nConsider you have a hypothesis you wish to test (alternative hypothesis). Using data collected, you express the rules of the game through probabilities, distributions, and statistical models. This helps create a “toy model” of the world based on your hypothesis. The null hypothesis is then the “default world,” and your working hypothesis is literally everything else. You pretend you know how things work, and then check if reality agrees with you (hypothesis testing). Then you ask the big question: Does our data make the null hypothesis look completely ridiculous? If yes, we might just reject it and revise the null world accordingly.\nStatistics isn’t about finding the ultimate truth, but about making the best possible decisions with the information available. We estimate, test, and adjust—because in the end, being less wrong is the best we can do. In the words of the famous statistician George Box: “All models are wrong, but some are useful”.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Statistics?</span>"
    ]
  },
  {
    "objectID": "introduction/what-is-stats.html#the-art-of-estimation-guessing-but-smartly",
    "href": "introduction/what-is-stats.html#the-art-of-estimation-guessing-but-smartly",
    "title": "1  What is Statistics?",
    "section": "1.3 The Art of Estimation: Guessing, but Smartly 🧐",
    "text": "1.3 The Art of Estimation: Guessing, but Smartly 🧐\nIf we had perfect information, we’d never need statistics. But in reality, we almost never have all the facts. Instead, we deal with:\n\nWhat we wish we knew (the population)\nWhat we actually know (our sample)\nWhat we do in response (estimate!)\n\nSince we can’t observe the entire universe, we estimate parameters from limited data. It’s like trying to guess the total number of jellybeans in a jar after grabbing only a handful—you’ll never be 100% sure, but a good estimate is better than a wild guess.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Statistics?</span>"
    ]
  },
  {
    "objectID": "introduction/what-is-stats.html#using-math-to-wrangle-uncertainty",
    "href": "introduction/what-is-stats.html#using-math-to-wrangle-uncertainty",
    "title": "1  What is Statistics?",
    "section": "1.4 Using Math to Wrangle Uncertainty 📊",
    "text": "1.4 Using Math to Wrangle Uncertainty 📊\nMathematics helps us express the rules of the game—through probabilities, distributions, and statistical models. We create a “toy model” of the world, pretending we know how things work, and then check if reality agrees with us.\nIn short: - We combine data with assumptions to make decisions 🔢 - We use probability to describe the world under the null hypothesis 🎲 - There is no magic formula that turns uncertainty into certainty—just better estimates!",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Statistics?</span>"
    ]
  },
  {
    "objectID": "introduction/what-is-stats.html#conclusion-embrace-the-uncertainty",
    "href": "introduction/what-is-stats.html#conclusion-embrace-the-uncertainty",
    "title": "1  What is Statistics?",
    "section": "1.5 Conclusion: Embrace the Uncertainty!",
    "text": "1.5 Conclusion: Embrace the Uncertainty!\nStatistics isn’t about finding the ultimate truth, but about making the best possible decisions with the information available. We estimate, test, and adjust—because in the end, being less wrong is the best we can do! 🚀📈",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Statistics?</span>"
    ]
  },
  {
    "objectID": "introduction/what-is-stats.html#this-book",
    "href": "introduction/what-is-stats.html#this-book",
    "title": "1  What is Statistics?",
    "section": "1.2 This Book",
    "text": "1.2 This Book\nThe Statistical Inference Pipeline”\nThis diagram illustrates the statistical pipeline, a structured process for drawing conclusions from data: 1. Producing Data – A sample is extracted from the population. 2. Descriptive Analysis – The sample is summarized using descriptive statistics. 3. Probability Modeling – Statistical models estimate the relationship between the sample and the population. 4. Inference – Conclusions about the entire population are made based on the sample data.\nThis pipeline is fundamental in statistics, enabling researchers to make data-driven decisions with limited information.\n“The Statistical Data Pipeline”\nThis diagram represents the pipeline of statistical analysis, outlining the key steps in drawing insights from data: 1. Producing Data – A sample is selected from the population to represent the whole. 2. Descriptive Statistics – The sample is analyzed to summarize patterns and trends. 3. Probability Modeling – Statistical methods establish connections between the sample and population. 4. Inference – Findings from the sample are generalized to make conclusions about the population.\nThis structured pipeline is essential in statistics, ensuring reliable and data-driven decision-making. The analysis process is visualized in Figure 1.1.\n\n\n\n\n\n\n\nFigure 1.1: The Statistical Data Pipeline\n\n\n\n\nWe will cover each of the steps depicted in Figure 1.1 in this book. Step 1 refers to the data collection phase which is covered in chapter The sample is analyzed to summarize key characteristics….. Step 2 is on the xploratory analysis of data and is covered in chapters…. Next is the rhid step concerning the relationship between sample data and the population is modeled. is covered in chaopters.. and finally, inferential statistic where we Conclusions are drawn about the population based on the sample.",
    "crumbs": [
      "Introduction",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>What is Statistics?</span>"
    ]
  }
]