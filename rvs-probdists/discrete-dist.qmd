## Discrete Probability Distributions

A **discrete probability distribution** describes how the values of a variable are associated with probabilities. It applies when a variable can take on a limited or countable set of values, like 0, 1, 2, and so on, and we know how likely each value is. Note that we use uppercase letter to denote random variables and lowercase letters for the values they take on.

The probability function $P(x)$ assigns a probability to each possible value $x$ that the variable can take. These probabilities must all lie between 0 and 1, and together they must sum to exactly 1.

We use the expected value, $\mathbb{E}(X)$, to summarize the typical or central value of the distribution. It gives us a sense of where the values tend to cluster. Think of it as a kind of weighted average, where more probable outcomes count more heavily in the calculation.

The variance, $\mathbb{V}(X)$, describes how much the values of the variable differ from this central value. A small variance means most values are close to the expected value, while a large variance means the values are more spread out.


## Probability Mass Function (PMF)
When dealing with discrete random variables, we are interested in how likely it is that the variable takes on specific values. This relationship, the link between possible values and their associated probabilities, is described by the **probability distribution** of the random variable.

The **probability mass function (PMF)** is a mathematical function that assigns a probability to each value the variable can take. In other words, the PMF is the mathematical expression of the probability distribution. The distribution is the overall concept; the PMF is the function that specifies the details.

We usually denote the PMF as $P(x)$, where:

$$
P(x) = P(X = x)
$$
This gives the probability that the random variable $X$ equals a specific value $x$.

For the PMF to describe a valid distribution, it must satisfy two conditions:

- Each probability must be between 0 and 1:
  $$
  0 \leq P(x) \leq 1
  $$

- The total probability across all values must sum to 1:
  $$
  \sum_x P(x) = 1
  $$


If we know $P(x)$ for all values $x$ that $X$ can take, we say we know the probability distribution of $X$.
We can present a probability distribution in several forms: as a table, a bar chart, or through a formula.


#### Example 12.1: Coin Toss  {.unnumbered}
Suppose we toss a fair coin two times. Let the random variable $X$ represent the number of heads observed. The sample space consists of four equally likely outcomes:

- Head, Head → $X = 2$
- Head, Tail → $X = 1$
- Tail, Head → $X = 1$
- Tail, Tail → $X = 0$

Each of these outcomes has probability 0.25. From this, we can define the probability mass function (PMF) of $X$ as:

$$
P(x) =
\begin{cases}
0.25 & \text{if } x = 0 \\
0.50 & \text{if } x = 1 \\
0.25 & \text{if } x = 2 \\
0 & \text{otherwise}
\end{cases}
$$

The table below presents a representation of the PMF; showing the values of $P(x)$ for each value in the support of $X$:

| Outcomes               | $x$ (Number of heads) | $P(x)$ |
|------------------------|------------------------|--------|
| (Tail, Tail)             | 0                      | 0.25   |
| (Head, Tail), (Tail, Head)             | 1                      | 0.50   |
| (Head, Head)             | 2                      | 0.25   |

We can confirm that this satisfies the requirement:

$$
\sum_x P(x) = 0.25 + 0.5 + 0.25 = 1
$$

This confirms that $P(x)$ defines a valid probability distribution.


This table is one way of representing the probability distribution of $X$. Another way to represent it is by using bar plot:

```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 5
#| fig-align: center
library(ggplot2)

# Define values and probabilities
df_pmf <- data.frame(
  x = c(0, 1, 2),
  p = c(0.25, 0.5, 0.25)
)

# PMF plot
ggplot(df_pmf, aes(x = factor(x), y = p)) +
  geom_col(fill = "grey") +
  labs(
    title = "Probability Mass Function of X",
    x = "x",
    y = expression(P(x))
  ) +
  ylim(0, 0.6) +
  theme_minimal()
```


##  The Cumulative Distribution Function (CDF)

For a discrete random variable $X$, we can describe its probability distribution not only using the probability mass function, but also through its **cumulative distribution function**, denoted $F(x)$.

The CDF gives the probability that $X$ takes on a value less than or equal to a given number $x$:

$$
F(x) = P(X \leq x)
$$

This function grows step by step as we move through the possible values of $X$, accumulating the total probability up to each point.

#### Example 12.1: Coin Toss (continued) {.unnumbered}

We continue here with the example abive, where $X$ is the number of heads in two tosses of a fair coin. The PMF and corresponding CDF values are:

| $x$ | $P(x)$ | $F(x)$ |
|-----|--------|--------|
| 0   | 0.25   | 0.25   |
| 1   | 0.50   | 0.75   |
| 2   | 0.25   | 1.00   |

The last value of $F(x)$ must always equal 1, since the total probability must sum to 1.

```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 5
#| fig-align: center

# Values and PMF
df <- data.frame(
  x = c(0, 1, 2),
  p = c(0.25, 0.5, 0.25)
)

# Add cumulative probabilities
df$F_x <- cumsum(df$p)

# CDF Step Plot
ggplot(df, aes(x = x, y = F_x)) +
  geom_step(direction = "hv", linewidth = 1.2, color = "darkgrey") +
  geom_point(size = 4, color = "indianred") +
  labs(
    title = "Cumulative Distribution Function F(x)",
    x = "x",
    y = expression(F(x))
  ) +
  ylim(0, 1.05) +
  theme_minimal()
```

## Expected Value of a Discrete Random Variable

Just like empirical data can be summarized with averages and standard deviations, a discrete probability distribution can also be described using corresponding statistical measures. These include measures of central tendency, such as the expected value, and measures of variability, such as the standard deviation.

The **expected value** plays the role of a mean or average. But because the possible outcomes may occur with different probabilities, we must take this into account by assigning more weight to more likely outcomes. This means the values are weighted by their associated probabilities.

To find the expected value of a discrete random variable, we do not simply average the outcomes. Instead, we compute a weighted average where the weights are given by the probability mass function.

The expected value of a discrete random variable $X$ is denoted by $\mathbb{E}(X)$, where $\mathbb{E}$ stands for expectation.
It is defined as:

$$
\mathbb{E}(X) = \sum_{x} x \cdot P(x) = \mu_X
$$

This formula tells us to multiply each value $x$ by its probability $P(x)$ and then sum the results over all possible values of $X$.
The result, $\mathbb{E}(X)$, gives us the value we expect to observe on average in the long run, if we were to randomly select a value of $X$ according to its probability distribution.

The expected value represents the "center" of the distribution — the point where the values balance, considering how often each one occurs. For example, if a random variable has most of its probability mass near 1, its expected value will be close to 1.


It’s important to note that expected value doesn’t predict what will happen* — it predicts what happens on average.
It’s like asking, *“What would I get if the universe reran this scenario a million times?”*



#### Example 12.1: Coin Toss (continued) {.unnumbered}

If you revisit the coin-toss example where $X$ counts the number of heads in two tosses, you would compute the expected value as:

$$
\mathbb{E}(X) = \sum_{x} x \cdot P(x) = 0 \cdot 0.25 + 1 \cdot 0.5 + 2 \cdot 0.25 = 1
$$

This confirms that we expect to get 1 head on average when tossing a coin twice.

#### Example 12.2: Lottery -  A Risky Business  {.unnumbered}
Let’s say you're eyeing a lottery with 100 tickets. Each ticket costs 1€, and you're feeling lucky. The prize setup is:

- One lucky winner gets 50€    
- Three people win 10€    
- Five folks get 2€  
- And... the remaining 91 get absolutely nothing 

Let’s define a random variable $X$ as the payout of a randomly selected ticket.

So what values can $X$ take?

$$
X \in \{0, 2, 10, 50\}
$$

But — and here’s the key — just averaging those values like this:

$$
\frac{0 + 2 + 10 + 50}{4} = 15.5
$$

...makes no sense! That would only be correct if all outcomes were equally likely, which they're definitely not. Most people walk away with nothing. So we need to weight each value by how often it actually happens.

Let’s build a table to show how many tickets correspond to each prize:

| Prize (€) | Number of tickets | Probability $P(x)$ |
|----------------|-------------------|--------------------|
| 0              | 91                | 0.91               |
| 2              | 5                 | 0.05               |
| 10             | 3                 | 0.03               |
| 50             | 1                 | 0.01               |

So now we have a complete probability distribution for $X$ which is visualized below:

```{r}
#| echo: false
#| fig-height: 3
#| fig-width: 5
#| fig-align: center
library(tibble)
library(ggplot2)

# Data for lottery example
df_lottery <- tibble(
  prize = c(0, 2, 10, 50),
  probability = c(0.91, 0.05, 0.03, 0.01)
)

# Plot PMF
ggplot(df_lottery, aes(x = factor(prize), y = probability)) +
  geom_col(fill = "goldenrod") +
  labs(
    title = "Probability Distribution of Lottery Payout",
    x = "Payout (€)",
    y = expression(P(x))
  ) +
  theme_minimal()
```

Let’s compute the expected value $\mathbb{E}(X)$ using the PMF:

$$
\mathbb{E}(X) = \sum_x x \cdot P(x) = 0 \cdot 0.91 + 2 \cdot 0.05 + 10 \cdot 0.03 + 50 \cdot 0.01 = 0.90
$$

So even though one person might win 50€, the average payout per ticket is just 90  cents. That means if you pay 1€ per ticket, you're losing 10 cents on average.

Most of the time, lottery isn’t just random — it’s rigged (in favor of the organizers).
